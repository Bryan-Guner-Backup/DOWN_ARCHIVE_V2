{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Daily_Report_Calculation_Final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z774PSkAveGP",
        "colab_type": "text"
      },
      "source": [
        "# Daily Report Generation\n",
        "\n",
        "Our previous notebook with this code was half exploration, which was no longer needed.  This notebook contains all the code for generating our daily reports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2A-uTdLFdnP",
        "colab_type": "text"
      },
      "source": [
        "## Run Once\n",
        "\n",
        "The following cells are function and class definitions, and only need to be run once in order to generate reports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfNvb0CAuQ3M",
        "colab_type": "code",
        "outputId": "20c5253e-31dc-4965-fd02-456dae855e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Used in many places\n",
        "import pandas as pd\n",
        "import psycopg2 as pg\n",
        "from psycopg2.extras import execute_batch\n",
        "\n",
        "# Used to enter database credentials without saving them to the notebook file\n",
        "import getpass\n",
        "\n",
        "# Used to easily read in bus location data\n",
        "import pandas.io.sql as sqlio\n",
        "\n",
        "# Only used in the schedule class definition\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Used to find distances between lat/lon points, and match closest stops\n",
        "from math import sqrt, cos\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Used at the end, to convert the final product to JSON\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XwuEdJhJ4AS",
        "colab_type": "text"
      },
      "source": [
        "If you ever get this error, you will need to re-run this cell to reconnect to the database:\n",
        "\n",
        "`InternalError: current transaction is aborted, commands ignored until end of transaction block`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrXXS59_uSMB",
        "colab_type": "code",
        "outputId": "71190f2e-1d88-43c4-efcf-47957e4b852b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Enter database credentials.  Requires you to paste in the user and\n",
        "# password so it isn't saved in the notebook file\n",
        "print(\"Enter database username:\")\n",
        "user = getpass.getpass()\n",
        "print(\"Enter database password:\")\n",
        "password = getpass.getpass()\n",
        "\n",
        "creds = {\n",
        "  'user': user,\n",
        "  'password': password,\n",
        "  'host': \"lambdalabs24sfmta.cykkiwxbfvpg.us-east-1.rds.amazonaws.com\",\n",
        "  'dbname': \"historicalTransitData\"\n",
        "}\n",
        "\n",
        "# Set up connection to database\n",
        "cnx = pg.connect(**creds)\n",
        "cursor = cnx.cursor()\n",
        "\n",
        "print('\\nDatabase connection successful')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter database username:\n",
            "··········\n",
            "Enter database password:\n",
            "··········\n",
            "\n",
            "Database connection successful\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuOqPR6mJqCk",
        "colab_type": "text"
      },
      "source": [
        "### Schedule class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD5I_fodu1VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Schedule class definition\n",
        "# Copied from previous work, has extra methods that are not all used in this notebook\n",
        "\n",
        "class Schedule:\n",
        "  \"\"\"\n",
        "  The Schedule class loads schedule data for a given route and date\n",
        "\n",
        "  Attributes:\n",
        "      route_id (str): the id of the route loaded\n",
        "      date (pd.Timestamp): the date of the schedule loaded\n",
        "      route_data (dict): the raw schedule data\n",
        "      inbound_table (pd.DataFrame): a dataframe of the inbound schedule\n",
        "      outbound_table (pd.DataFrame): a dataframe of the outbound schedule\n",
        "      mean_interval (float): the average time in minutes between each \n",
        "                              scheduled stop\n",
        "      common_interval (float): the most common time (mode) in minutes between \n",
        "                                each scheduled stop\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, route_id, date, connection):\n",
        "      \"\"\"\n",
        "      The Schedule class loads the schedule for a particular route and day,\n",
        "      and makes several accessor methods available for it.\n",
        "\n",
        "      Parameters:\n",
        "\n",
        "      route_id (str or int)\n",
        "          - The route id to load\n",
        "\n",
        "      date (str or pandas.Timestamp)\n",
        "          - Which date to load\n",
        "          - Converted with pandas.to_datetime so many formats are acceptable\n",
        "        \n",
        "      connection (psycopg2 connection object)\n",
        "          - The connection object to connect to the database with\n",
        "      \"\"\"\n",
        "\n",
        "      self.route_id = str(route_id)\n",
        "      self.date = pd.to_datetime(date)\n",
        "\n",
        "      # load the schedule for that date and route\n",
        "      self.route_data = load_schedule(self.route_id, self.date, connection)\n",
        "\n",
        "      # process data into a table\n",
        "      self.inbound_table, self.outbound_table = extract_schedule_tables(self.route_data)\n",
        "\n",
        "      # calculate the common interval values\n",
        "      self.mean_interval, self.common_interval = get_common_intervals(\n",
        "                                  [self.inbound_table, self.outbound_table])\n",
        "\n",
        "  def list_stops(self):\n",
        "      \"\"\"\n",
        "      returns the list of all stops used by this schedule\n",
        "      \"\"\"\n",
        "\n",
        "      # get stops for both inbound and outbound routes\n",
        "      inbound = list(self.inbound_table.columns)\n",
        "      outbound = list(self.outbound_table.columns)\n",
        "\n",
        "      # convert to set to ensure no duplicates,\n",
        "      # then back to list for the correct output type\n",
        "      return list(set(inbound + outbound))\n",
        "\n",
        "  def get_specific_interval(self, stop, time, inbound=True):\n",
        "        \"\"\"\n",
        "        Returns the expected interval, in minutes, for a given stop and\n",
        "        time of day.\n",
        "\n",
        "        Parameters:\n",
        "\n",
        "        stop (str or int)\n",
        "            - the stop tag/id of the bus stop to check\n",
        "\n",
        "        time (str or pandas.Timestamp)\n",
        "            - the time of day to check, uses pandas.to_datetime to convert\n",
        "            - examples that work: \"6:00\", \"3:30pm\", \"15:30\"\n",
        "\n",
        "        inbound (bool, optional)\n",
        "            - whether to check the inbound or outbound schedule\n",
        "            - ignored unless the given stop is in both inbound and outbound\n",
        "        \"\"\"\n",
        "\n",
        "        # ensure correct parameter types\n",
        "        stop = str(stop)\n",
        "        time = pd.to_datetime(time)\n",
        "\n",
        "        # check which route to use, and extract the column for the given stop\n",
        "        if (stop in self.inbound_table.columns and\n",
        "        stop in self.outbound_table.columns):\n",
        "            # stop exists in both, use inbound parameter to decide\n",
        "            if inbound:\n",
        "                sched = self.inbound_table[stop]\n",
        "            else:\n",
        "                sched = self.outbound_table[stop]\n",
        "        elif (stop in self.inbound_table.columns):\n",
        "            # stop is in the inbound schedule, use that\n",
        "            sched = self.inbound_table[stop]\n",
        "        elif (stop in self.outbound_table.columns):\n",
        "            # stop is in the outbound schedule, use that\n",
        "            sched = self.outbound_table[stop]\n",
        "        else:\n",
        "            # stop doesn't exist in either, throw an error\n",
        "            raise ValueError(f\"Stop id '{stop}' doesn't exist in either inbound or outbound schedules\")\n",
        "\n",
        "        # 1: convert schedule to datetime for comparison statements\n",
        "        # 2: drop any NaN values\n",
        "        # 3: convert to list since pd.Series threw errors on i indexing\n",
        "        sched = list(pd.to_datetime(sched).dropna())\n",
        "\n",
        "        # reset the date portion of the time parameter to\n",
        "        # ensure we are checking the schedule correctly\n",
        "        time = time.replace(year=self.date.year, month=self.date.month,\n",
        "                            day=self.date.day)\n",
        "\n",
        "        # iterate through that list to find where the time parameter fits\n",
        "        for i in range(1, len(sched)):\n",
        "            # start at 1 and move forward,\n",
        "            # is the time parameter before this schedule entry?\n",
        "            if(time < sched[i]):\n",
        "                # return the difference between this entry and the previous one\n",
        "                return (sched[i] - sched[i-1]).seconds / 60\n",
        "\n",
        "        # can only reach this point if the time parameter is after all entries\n",
        "        # in the schedule, return the last available interval\n",
        "        return (sched[len(sched)-1] - sched[len(sched)-2]).seconds / 60\n",
        "\n",
        "\n",
        "def load_schedule(route, date, connection):\n",
        "    \"\"\"\n",
        "    loads schedule data from the database and returns it\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "        route (str)\n",
        "            - The route id to load\n",
        "\n",
        "        date (str or pd.Timestamp)\n",
        "            - Which date to load\n",
        "            - Converted with pandas.to_datetime so many formats are acceptable\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure correct parameter types\n",
        "    route = str(route)\n",
        "    date = pd.to_datetime(date)\n",
        "\n",
        "    # DB connection\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # build selection query\n",
        "    query = \"\"\"\n",
        "        SELECT content\n",
        "        FROM schedules\n",
        "        WHERE rid = %s AND\n",
        "            begin_date <= %s::TIMESTAMP AND\n",
        "            (end_date IS NULL OR end_date >= %s::TIMESTAMP);\n",
        "    \"\"\"\n",
        "\n",
        "    # execute query and save the route data to a local variable\n",
        "    cursor.execute(query, (route, str(date), str(date)))\n",
        "    if cursor.rowcount == 0:\n",
        "      raise Exception(f\"No schedule data found for route {route} on {date.date()}\")\n",
        "    \n",
        "    data = cursor.fetchone()[0]['route']\n",
        "\n",
        "    # pd.Timestamp.dayofweek returns 0 for monday and 6 for Sunday\n",
        "    # the actual serviceClass strings are defined by Nextbus\n",
        "    # these are the only 3 service classes we can currently observe,\n",
        "    # if others are published later then this will need to change\n",
        "    if(date.dayofweek <= 4):\n",
        "        serviceClass = 'wkd'\n",
        "    elif(date.dayofweek == 5):\n",
        "        serviceClass = 'sat'\n",
        "    else:\n",
        "        serviceClass = 'sun'\n",
        "\n",
        "    # the schedule format has two entries for each serviceClass,\n",
        "    # one each for inbound and outbound.\n",
        "\n",
        "    # get each entry in the data list with the correct serviceClass\n",
        "    result = [sched for sched in data if (sched['serviceClass'] == serviceClass)]\n",
        "\n",
        "    # make sure there's data \n",
        "    # (most commonly reason to be here: some routes don't run on weekends)\n",
        "    if len(result) == 0:\n",
        "      print(f\"No schedule data found for route {route} on {date.date()}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def extract_schedule_tables(route_data):\n",
        "    \"\"\"\n",
        "    converts raw schedule data to two pandas dataframes\n",
        "\n",
        "    columns are stops, and rows are individual trips\n",
        "\n",
        "    returns inbound_df, outbound_df\n",
        "    \"\"\"\n",
        "    \n",
        "    # assuming 2 entries, but not assuming order\n",
        "    if(route_data[0]['direction'] == 'Inbound'):\n",
        "        inbound = 0\n",
        "    else:\n",
        "        inbound = 1\n",
        "\n",
        "    # extract a list of stops to act as columns\n",
        "    inbound_stops = [s['tag'] for s in route_data[inbound]['header']['stop']]\n",
        "\n",
        "    # initialize dataframe\n",
        "    inbound_df = pd.DataFrame(columns=inbound_stops)\n",
        "\n",
        "    # extract each row from the data\n",
        "    if type(route_data[inbound]['tr']) == list:\n",
        "        # if there are multiple trips in a day, structure will be a list\n",
        "        i = 0\n",
        "        for trip in route_data[inbound]['tr']:\n",
        "            for stop in trip['stop']:\n",
        "                # '--' indicates the bus is not going to that stop on this trip\n",
        "                if stop['content'] != '--':\n",
        "                    inbound_df.at[i, stop['tag']] = stop['content']\n",
        "            # increment for the next row\n",
        "            i += 1\n",
        "    else:\n",
        "        # if there is only 1 trip in a day, the object is a dict and\n",
        "        # must be handled slightly differently\n",
        "        for stop in route_data[inbound]['tr']['stop']:\n",
        "            if stop['content'] != '--':\n",
        "                    inbound_df.at[0, stop['tag']] = stop['content']\n",
        "\n",
        "    # flip between 0 and 1\n",
        "    outbound = int(not inbound)\n",
        "\n",
        "    # repeat steps for the outbound schedule\n",
        "    outbound_stops = [s['tag'] for s in route_data[outbound]['header']['stop']]\n",
        "    outbound_df = pd.DataFrame(columns=outbound_stops)\n",
        "\n",
        "    if type(route_data[outbound]['tr']) == list:\n",
        "        i = 0\n",
        "        for trip in route_data[outbound]['tr']:\n",
        "            for stop in trip['stop']:\n",
        "                if stop['content'] != '--':\n",
        "                    outbound_df.at[i, stop['tag']] = stop['content']\n",
        "            i += 1\n",
        "    else:\n",
        "        for stop in route_data[outbound]['tr']['stop']:\n",
        "            if stop['content'] != '--':\n",
        "                    outbound_df.at[0, stop['tag']] = stop['content']\n",
        "\n",
        "    # return both dataframes\n",
        "    return inbound_df, outbound_df\n",
        "\n",
        "\n",
        "def get_common_intervals(df_list):\n",
        "    \"\"\"\n",
        "    takes route schedule tables and returns both the average interval (mean)\n",
        "    and the most common interval (mode), measured in number of minutes\n",
        "\n",
        "    takes a list of dataframes and combines them before calculating statistics\n",
        "\n",
        "    intended to combine inbound and outbound schedules for a single route\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure we have at least one dataframe\n",
        "    if len(df_list) == 0:\n",
        "        raise ValueError(\"Function requires at least one dataframe\")\n",
        "\n",
        "    # append all dataframes in the array together\n",
        "    df = df_list[0].copy()\n",
        "    for i in range(1, len(df_list)):\n",
        "        df.append(df_list[i].copy())\n",
        "\n",
        "    # convert all values to datetime so we can get an interval easily\n",
        "    for col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col])\n",
        "\n",
        "    # initialize a table to hold each individual interval\n",
        "    intervals = pd.DataFrame(columns=df.columns)\n",
        "    intervals['temp'] = range(len(df))\n",
        "\n",
        "    # take each column and find the intervals in it\n",
        "    for col in df.columns:\n",
        "        prev_time = np.nan\n",
        "        for i in range(len(df)):\n",
        "            # find the first non-null value and save it to prev_time\n",
        "            if pd.isnull(prev_time):\n",
        "                prev_time = df.at[i, col]\n",
        "            # if the current time is not null, save the interval\n",
        "            elif ~pd.isnull(df.at[i, col]):\n",
        "                intervals.at[i, col] = (df.at[i, col] - prev_time).seconds / 60\n",
        "                prev_time = df.at[i, col]\n",
        "\n",
        "    # this runs without adding a temp column, but the above loop runs 3x as\n",
        "    # fast if the rows already exist\n",
        "    intervals = intervals.drop('temp', axis=1)\n",
        "\n",
        "    # calculate the mean of the entire table\n",
        "    mean = intervals.mean().mean()\n",
        "\n",
        "    # calculate the mode of the entire table, the [0][0] at the end is\n",
        "    # because scipy.stats returns an entire ModeResult class\n",
        "    mode = stats.mode(intervals.values.flatten())[0][0]\n",
        "\n",
        "    return mean, mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nw0rmh7Jt-r",
        "colab_type": "text"
      },
      "source": [
        "### Route class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJzK7mePwLD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Route class definition\n",
        "# Copied from previous work, has extra methods that are not all used in this notebook\n",
        "\n",
        "class Route:\n",
        "  def __init__(self, route_id, date, connection):\n",
        "    \"\"\"\n",
        "    The Route class loads the route configuration data for a particular\n",
        "    route, and makes several accessor methods available for it.\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "    route_id (str or int)\n",
        "        - The route id to load\n",
        "\n",
        "    date (str or pandas.Timestamp)\n",
        "        - Which date to load\n",
        "        - Converted with pandas.to_datetime so many formats are acceptable\n",
        "    \n",
        "    connection (psycopg2 connection object)\n",
        "        - The connection object to connect to the database with\n",
        "    \"\"\"\n",
        "\n",
        "    self.route_id = str(route_id)\n",
        "    self.date = pd.to_datetime(date)\n",
        "\n",
        "    # load the route data\n",
        "    self.route_data, self.route_type, self.route_name = load_route(self.route_id, self.date, connection)\n",
        "\n",
        "    # extract stops table\n",
        "    self.stops_table, self.inbound, self.outbound = extract_stops(self.route_data)\n",
        "\n",
        "    # The extract_path method is not complete\n",
        "\n",
        "    # extract route path, list of (lat, lon) pairs\n",
        "    # self.path_coords = extract_path(self.route_data)\n",
        "\n",
        "\n",
        "def load_route(route, date, connection):\n",
        "    \"\"\"\n",
        "    loads raw route data from the database\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "        route (str or int)\n",
        "            - The route id to load\n",
        "\n",
        "        date (str or pd.Timestamp)\n",
        "            - Which date to load\n",
        "            - Converted with pandas.to_datetime so many formats are acceptable\n",
        "    \n",
        "    Returns route_data (dict), route_type (str), route_name (str)\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure correct parameter types\n",
        "    route = str(route)\n",
        "    date = pd.to_datetime(date)\n",
        "\n",
        "    # DB connection\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # build selection query\n",
        "    query = \"\"\"\n",
        "        SELECT route_name, route_type, content\n",
        "        FROM routes\n",
        "        WHERE rid = %s AND\n",
        "            begin_date <= %s::TIMESTAMP AND\n",
        "            (end_date IS NULL OR end_date > %s::TIMESTAMP);\n",
        "    \"\"\"\n",
        "\n",
        "    # execute query and return the route data\n",
        "    cursor.execute(query, (route, str(date), str(date)))\n",
        "    if cursor.rowcount == 0:\n",
        "      raise Exception(f\"No route data found for route {route} on {date.date()}\")\n",
        "\n",
        "    result = cursor.fetchone()\n",
        "    return result[2]['route'], result[1], result[0]\n",
        "\n",
        "\n",
        "def extract_path(route_data):\n",
        "    \"\"\"\n",
        "    Extracts the list of path coordinates for a route.\n",
        "\n",
        "    The raw data stores this as an unordered list of sub-routes, so this\n",
        "    function deciphers the order they should go in and returns a single list.\n",
        "    \"\"\"\n",
        "\n",
        "    # KNOWN BUG\n",
        "    # this approach assumed all routes were either a line or a loop.\n",
        "    # routes that have multiple sub-paths meeting at a point break this,\n",
        "    # route 24 is a current example.\n",
        "    # I'm committing this now to get the rest of the code out there\n",
        "    \n",
        "    # this part is also not currently used in the daily report generation\n",
        "\n",
        "    # extract the list of subpaths as just (lat,lon) coordinates\n",
        "    # also converts from string to float (raw data has strings)\n",
        "    path = []\n",
        "    for sub_path in route_data['path']:\n",
        "        path.append([(float(p['lat']), float(p['lon'])) \n",
        "                     for p in sub_path['point']])\n",
        "\n",
        "    # start with the first element, remove it from path\n",
        "    final = path[0]\n",
        "    path.pop(0)\n",
        "\n",
        "    # loop until the first and last coordinates in final match\n",
        "    counter = len(path)\n",
        "    done = True\n",
        "    while final[0] != final[-1]:\n",
        "        # loop through the sub-paths that we haven't yet moved to final\n",
        "        for i in range(len(path)):\n",
        "            # check if the last coordinate in final matches the first \n",
        "            # coordinate of another sub-path\n",
        "            if final[-1] == path[i][0]:\n",
        "                # match found, move it to final\n",
        "                # leave out the first coordinate to avoid duplicates\n",
        "                final = final + path[i][1:]\n",
        "                path.pop(i)\n",
        "                break  # break the for loop\n",
        "                \n",
        "        # protection against infinite loops, if the path never closes\n",
        "        counter -= 1\n",
        "        if counter < 0:\n",
        "            done = False\n",
        "            break\n",
        "\n",
        "    if not done:\n",
        "        # route did not connect in a loop, perform same steps backwards \n",
        "        # to get the rest of the line\n",
        "        for _ in range(len(path)):\n",
        "            # loop through the sub-paths that we haven't yet moved to final\n",
        "            for i in range(len(path)):\n",
        "                # check if the first coordinate in final matches the last \n",
        "                # coordinate of another sub-path\n",
        "                if final[0] == path[i][-1]:\n",
        "                    # match found, move it to final\n",
        "                    # leave out the last coordinate to avoid duplicates\n",
        "                    final = path[i][:-1] + final\n",
        "                    path.pop(i)\n",
        "                    break  # break the for loop\n",
        "\n",
        "    # some routes may have un-used sub-paths\n",
        "    # Route 1 for example has two sub-paths that are almost identical, with the \n",
        "    # same start and end points\n",
        "    # if len(path) > 0:\n",
        "    #     print(f\"WARNING: {len(path)} unused sub-paths\")\n",
        "\n",
        "    # return the final result\n",
        "    return final\n",
        "\n",
        "\n",
        "def extract_stops(route_data):\n",
        "  \"\"\"\n",
        "  Extracts a dataframe of stops info\n",
        "\n",
        "  Returns the main stops dataframe, and a list of inbound and outbound stops \n",
        "  in the order they are intended to be on the route\n",
        "  \"\"\"\n",
        "\n",
        "  stops = pd.DataFrame(route_data['stop'])\n",
        "  directions = pd.DataFrame(route_data['direction'])\n",
        "\n",
        "  # Change stop arrays to just the list of numbers\n",
        "  for i in range(len(directions)):\n",
        "    directions.at[i, 'stop'] = [s['tag'] for s in directions.at[i, 'stop']]\n",
        "\n",
        "  # Find which stops are inbound or outbound\n",
        "  inbound = []\n",
        "  for stop_list in directions[directions['name'] == \"Inbound\"]['stop']:\n",
        "    for stop in stop_list:\n",
        "      if stop not in inbound:\n",
        "        inbound.append(stop)\n",
        "\n",
        "  outbound = []\n",
        "  for stop_list in directions[directions['name'] == \"Outbound\"]['stop']:\n",
        "    for stop in stop_list:\n",
        "      if stop not in inbound:\n",
        "        outbound.append(stop)\n",
        "\n",
        "  # Label each stop as inbound or outbound\n",
        "  stops['direction'] = ['none'] * len(stops)\n",
        "  for i in range(len(stops)):\n",
        "    if stops.at[i, 'tag'] in inbound:\n",
        "      stops.at[i, 'direction'] = 'inbound'\n",
        "    elif stops.at[i, 'tag'] in outbound:\n",
        "      stops.at[i, 'direction'] = 'outbound'\n",
        "\n",
        "  # Convert from string to float\n",
        "  stops['lat'] = stops['lat'].astype(float)\n",
        "  stops['lon'] = stops['lon'].astype(float)\n",
        "\n",
        "  return stops, inbound, outbound"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fKaClclJ9DT",
        "colab_type": "text"
      },
      "source": [
        "### Get bus location data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfyIVYcuy6eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_location_data(rid, begin, end, connection):\n",
        "  # Build query to select location data\n",
        "  query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM locations\n",
        "    WHERE rid = '{rid}' AND\n",
        "      timestamp > '{begin}'::TIMESTAMP AND\n",
        "      timestamp < '{end}'::TIMESTAMP\n",
        "    ORDER BY id;\n",
        "  \"\"\"\n",
        "\n",
        "  # read the query directly into pandas\n",
        "  locations = sqlio.read_sql_query(query, connection)\n",
        "\n",
        "  if len(locations) == 0:\n",
        "    raise Exception(f\"No bus location data found for route {rid} between {begin} and {end}\")\n",
        "\n",
        "  # Convert those UTC timestamps to local PST by subtracting 7 hours\n",
        "  locations['timestamp'] = locations['timestamp'] - pd.Timedelta(hours=7)\n",
        "\n",
        "  # return the result\n",
        "  return locations\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y99QhIMFJ_W9",
        "colab_type": "text"
      },
      "source": [
        "### Lat/lon distance formula"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi2_KkBuz8-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fcc_projection(loc1, loc2):\n",
        "    \"\"\"\n",
        "    function to apply FCC recommended formulae\n",
        "    for calculating distances on earth projected to a plane\n",
        "    \n",
        "    significantly faster computationally, negligible loss in accuracy\n",
        "    \n",
        "    Args: \n",
        "    loc1 - a tuple of lat/lon\n",
        "    loc2 - a tuple of lat/lon\n",
        "    \"\"\"\n",
        "    lat1, lat2 = loc1[0], loc2[0]\n",
        "    lon1, lon2 = loc1[1], loc2[1]\n",
        "    \n",
        "    mean_lat = (lat1+lat2)/2\n",
        "    delta_lat = lat2 - lat1\n",
        "    delta_lon = lon2 - lon1\n",
        "    \n",
        "    k1 = 111.13209 - 0.56605*cos(2*mean_lat) + .0012*cos(4*mean_lat)\n",
        "    k2 = 111.41513*cos(mean_lat) - 0.09455*cos(3*mean_lat) + 0.00012*cos(5*mean_lat)\n",
        "    \n",
        "    distance = sqrt((k1*delta_lat)**2 + (k2*delta_lon)**2)\n",
        "    \n",
        "    return distance\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNa0AkVJKFiC",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning function\n",
        "\n",
        "- removes location reports older than 60 seconds\n",
        "- removes location reports with no direction value\n",
        "- shifts timestamps according to the age column, so location and timestamp match\n",
        "- uses the cdist function from scipy to quickly get all distances between each pair of location report and potential stop\n",
        "- saves the closest stop to each location report\n",
        "- Drop any rows where closest stop is too far away.  \n",
        "  - This makes sense for longer routes that can go for a few kilometers without a stop, such as route 25 over the golden gate bridge.\n",
        "  - Right now we are approximating by saying the closest stop is where the bus actually is at that time.  If we improve that approximation, we should probably stop dropping these rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XijxSQqjQ9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_locations(locations, stops):\n",
        "  \"\"\"\n",
        "  takes a dataframe of bus locations and a dataframe of \n",
        "\n",
        "  returns the locations dataframe with nearest stop added\n",
        "  \"\"\"\n",
        "  \n",
        "  # remove old location reports that would be duplicates\n",
        "  df = locations[locations['age'] < 60].copy()\n",
        "\n",
        "  # remove rows with no direction value\n",
        "  df = df[~pd.isna(df['direction'])]\n",
        "\n",
        "  # shift timestamps according to the age column\n",
        "  df['timestamp'] = df.apply(shift_timestamp, axis=1)\n",
        "\n",
        "  # Make lists of all inbound or outbound stops\n",
        "  inbound_stops = stops[stops['direction'] == 'inbound'].reset_index(drop=True)\n",
        "  outbound_stops = stops[stops['direction'] == 'outbound'].reset_index(drop=True)\n",
        "\n",
        "  # Assign closest stops\n",
        "  # separate inbound and outbound so we compare the right stops\n",
        "  df_in = df[df['direction'].str.contains('_I_')].copy()\n",
        "  df_out = df[df['direction'].str.contains('_O_')].copy()\n",
        "\n",
        "  # use scipy to find the closest stop to each location report\n",
        "  closest_in = cdist(df_in[['latitude', 'longitude']].to_numpy(),\n",
        "                     inbound_stops[['lat', 'lon']].to_numpy(),\n",
        "                     metric=fcc_projection)\n",
        "  closest_out = cdist(df_out[['latitude', 'longitude']].to_numpy(),\n",
        "                      outbound_stops[['lat', 'lon']].to_numpy(),\n",
        "                      metric=fcc_projection)\n",
        "  \n",
        "  # save results back to df\n",
        "  df_in['closestStop'] = [closest_in[i].argmin() for i in range(len(closest_in))]\n",
        "  df_in['distance'] = [closest_in[i].min() for i in range(len(closest_in))]\n",
        "  df_out['closestStop'] = [closest_out[i].argmin() for i in range(len(closest_out))]\n",
        "  df_out['distance'] = [closest_out[i].min() for i in range(len(closest_out))]\n",
        "\n",
        "  # cdist gives us the index of closestStop, convert it to the stop tag\n",
        "  df_in['closestStop'] = df_in['closestStop'].apply(\n",
        "      lambda x: int(inbound_stops.at[x, 'tag']))\n",
        "  df_out['closestStop'] = df_out['closestStop'].apply(\n",
        "      lambda x: int(outbound_stops.at[x, 'tag']))\n",
        "\n",
        "  # df_in and df_out back together and return it\n",
        "  df = df_in.append(df_out).sort_values(['timestamp', 'vid']).reset_index(drop=True)\n",
        "\n",
        "  # drop any rows that were more than .5 kilometers away from a stop\n",
        "  df = df[df['distance'] < .5]\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def shift_timestamp(row):\n",
        "  \"\"\" subtracts row['age'] from row['timestamp'] \"\"\"\n",
        "  return row['timestamp'] - pd.Timedelta(seconds=row['age'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBcZu_sGKnM0",
        "colab_type": "text"
      },
      "source": [
        "### Get stop times\n",
        "\n",
        "Creates a list of times that each stop sees a bus.  \n",
        "\n",
        "Fills in times for stops in between each location report (interpolation)\n",
        "\n",
        "![interpolation graphic](https://drive.google.com/uc?export=view&id=1lroClIK-i6_mysd5SA3mQCHsNKuy5t_r)\n",
        "\n",
        "Possible improvement: the current approach assumes equal distances between stops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIIAo9Pe74TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_stop_times(locations, route):\n",
        "  \"\"\"\n",
        "  returns a dict, keys are stop tags and values are lists of timestamps \n",
        "  that describe every time a bus was seen at that stop\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the data structure I will store results in\n",
        "  stop_times = {}\n",
        "  for stop in route.inbound + route.outbound:\n",
        "    stop_times[str(stop)] = []\n",
        "\n",
        "  for vid in locations['vid'].unique():\n",
        "    # Process the route one vehicle at a time\n",
        "    df = locations[locations['vid'] == vid]\n",
        "\n",
        "    # process 1st row on its own\n",
        "    prev_row = df.loc[df.index[0]]\n",
        "    stop_times[str(prev_row['closestStop'])].append(prev_row['timestamp'])\n",
        "\n",
        "    # loop through the rest of the rows, comparing each to the previous one\n",
        "    for i, row in df[1:].iterrows():\n",
        "      if row['direction'] != prev_row['direction']:\n",
        "        # changed directions, don't compare to previous row\n",
        "        stop_times[str(row['closestStop'])].append(row['timestamp'])\n",
        "      else:\n",
        "        # same direction, compare to previous row\n",
        "        if '_I_' in row['direction']:  # get correct stop list\n",
        "          stoplist = route.inbound\n",
        "        else:\n",
        "          stoplist = route.outbound\n",
        "\n",
        "        current = stoplist.index(str(row['closestStop']))\n",
        "        previous = stoplist.index(str(prev_row['closestStop']))\n",
        "        gap = current - previous\n",
        "        if gap > 1:  # need to interpolate\n",
        "          diff = (row['timestamp'] - prev_row['timestamp'])/gap\n",
        "          counter = 1\n",
        "          for stop in stoplist[previous+1:current]:\n",
        "            # save interpolated time\n",
        "            stop_times[str(stop)].append(prev_row['timestamp'] + (counter * diff))\n",
        "\n",
        "            # increase counter for the next stop\n",
        "            # example: with 2 interpolated stops, gap would be 3\n",
        "            # 1st diff is 1/3, next is 2/3\n",
        "            counter += 1\n",
        "        \n",
        "        if row['closestStop'] != prev_row['closestStop']:\n",
        "          # only save time if the stop has changed, \n",
        "          # otherwise the bus hasn't moved since last time\n",
        "          stop_times[str(row['closestStop'])].append(row['timestamp'])\n",
        "      \n",
        "      # advance for next row\n",
        "      prev_row = row\n",
        "  \n",
        "  # Sort each list before returning\n",
        "  for stop in stop_times.keys():\n",
        "    stop_times[stop].sort()\n",
        "\n",
        "  return stop_times\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoqfvNtxLC0_",
        "colab_type": "text"
      },
      "source": [
        "### Find all bunches and gaps\n",
        "\n",
        "Analzes the list of times each stop sees a bus, checks for time gaps that are too short or long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh5YpJtYBNP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bunches_gaps(stop_times, schedule, bunch_threshold=.2, gap_threshold=1.5):\n",
        "  \"\"\"\n",
        "  returns a dataframe of all bunches and gaps found\n",
        "\n",
        "  default thresholds define a bunch as 20% and a gap as 150% of scheduled headway\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize dataframe for the bunces and gaps\n",
        "  problems = pd.DataFrame(columns=['type', 'time', 'duration', 'stop'])\n",
        "  counter = 0\n",
        "\n",
        "  # Set the bunch/gap thresholds (in seconds)\n",
        "  bunch_threshold = (schedule.common_interval * 60) * bunch_threshold\n",
        "  gap_threshold = (schedule.common_interval * 60) * gap_threshold\n",
        "\n",
        "  for stop in stop_times.keys():\n",
        "    # ensure we have any times at all for this stop\n",
        "    if len(stop_times[stop]) == 0:\n",
        "      #print(f\"Stop {stop} had no recorded times\")\n",
        "      continue  # go to next stop in the loop\n",
        "\n",
        "    # save initial time\n",
        "    prev_time = stop_times[stop][0]\n",
        "\n",
        "    # loop through all others, comparing to the previous one\n",
        "    for time in stop_times[stop][1:]:\n",
        "      diff = (time - prev_time).seconds\n",
        "      if diff <= bunch_threshold:\n",
        "        # bunch found, save it\n",
        "        problems.at[counter] = ['bunch', prev_time, diff, stop]\n",
        "        counter += 1\n",
        "      elif diff >= gap_threshold:\n",
        "        problems.at[counter] = ['gap', prev_time, diff, stop]\n",
        "        counter += 1\n",
        "      \n",
        "      prev_time = time\n",
        "  \n",
        "  return problems\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbCJreP0LVbU",
        "colab_type": "text"
      },
      "source": [
        "### Calculate on-time percentage\n",
        "\n",
        "Goes through each scheduled stop and checks if a bus was there on time.\n",
        "\n",
        "SFMTA defines \"on time\" as \"within four minutes late or one minute early of the scheduled arrival time\"\n",
        "\n",
        "- source: https://www.sfmta.com/reports/muni-time-performance\n",
        "\n",
        "We don't have enough precision in our data to distinguish arrivals and departures from every specific stop, so our results may not match up exactly with SFMTA's reported results.  That website reports monthly statistics, but not daily.\n",
        "\n",
        "These approximations also do not have info on which bus is supposed to be which trip.  This code does not distinguish between early or late if a scheduled stop was missed, because we do not know if the bus before or after was supposed to make that stop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaOhrtrBD09m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this uses sequential search, could speed up with binary search if needed,\n",
        "# but it currently uses hardly any time in comparison to other steps\n",
        "def helper_count(expected_times, observed_times):\n",
        "  \"\"\" Returns the number of on-time stops found \"\"\"\n",
        "\n",
        "  # set up early/late thresholds (in seconds)\n",
        "  early_threshold = pd.Timedelta(seconds=1*60)  # 1 minute early\n",
        "  late_threshold = pd.Timedelta(seconds=4*60)   # 4 minutes late\n",
        "\n",
        "  count = 0\n",
        "  for stop in expected_times.columns:\n",
        "    for expected in expected_times[stop]:\n",
        "      if pd.isna(expected):\n",
        "        continue  # skip NaN values in the expected schedule\n",
        "\n",
        "      # for each expected time...\n",
        "      # find first observed time after the early threshold\n",
        "      found_time = None\n",
        "      early = expected - early_threshold\n",
        "\n",
        "      # BUG: some schedule data may have stop tags that are not in the inbound\n",
        "      # or outbound definitions for a route.  That would throw a key error here.\n",
        "      # Example: stop 14148 on route 24\n",
        "      # current solution is to ignore those stops with the try/except statement\n",
        "      try:\n",
        "        for observed in observed_times[stop]:\n",
        "          if observed >= early:\n",
        "            found_time = observed\n",
        "            break\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      # if found time is still None, then all observed times were too early\n",
        "      # if found_time is before the late threshold then we were on time\n",
        "      if (not pd.isna(found_time)) and found_time <= (expected + late_threshold):\n",
        "        # found_time is within the on-time window\n",
        "        count += 1\n",
        "\n",
        "  return count\n",
        "\n",
        "def calculate_ontime(stop_times, schedule):\n",
        "  \"\"\" Returns the on-time percentage and total scheduled stops for this route \"\"\"\n",
        "\n",
        "  # Save schedules with timestamp data types, set date to match\n",
        "  inbound_times = schedule.inbound_table\n",
        "  for col in inbound_times.columns:\n",
        "    inbound_times[col] = pd.to_datetime(inbound_times[col]).apply(\n",
        "        lambda dt: dt.replace(year=schedule.date.year, \n",
        "                              month=schedule.date.month, \n",
        "                              day=schedule.date.day))\n",
        "\n",
        "  outbound_times = schedule.outbound_table\n",
        "  for col in outbound_times.columns:\n",
        "    outbound_times[col] = pd.to_datetime(outbound_times[col]).apply(\n",
        "        lambda dt: dt.replace(year=schedule.date.year, \n",
        "                              month=schedule.date.month, \n",
        "                              day=schedule.date.day))\n",
        "  \n",
        "  # count times for both inbound and outbound schedules\n",
        "  on_time_count = (helper_count(inbound_times, stop_times) +\n",
        "                   helper_count(outbound_times, stop_times))\n",
        "  \n",
        "  # get total expected count\n",
        "  total_expected = inbound_times.count().sum() + outbound_times.count().sum()\n",
        "\n",
        "  # return on-time percentage\n",
        "  return (on_time_count / total_expected), total_expected\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuvuHfkELk7d",
        "colab_type": "text"
      },
      "source": [
        "### Generate line graph data\n",
        "\n",
        "Generates the graph data for the line graph showing bunches and gaps throughout the day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOyWGDKPSF8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bunch_gap_graph(problems, interval=10):\n",
        "  \"\"\"\n",
        "  returns data for a graph of the bunches and gaps throughout the day\n",
        "\n",
        "  problems - the dataframe of bunches and gaps\n",
        "\n",
        "  interval - the number of minutes to bin data into\n",
        "\n",
        "  returns\n",
        "  {\n",
        "    \"times\": [time values (x)],\n",
        "    \"bunches\": [bunch counts (y1)],\n",
        "    \"gaps\": [gap counts (y2)]\n",
        "  }\n",
        "  \"\"\"\n",
        "\n",
        "  # set the time interval\n",
        "  interval = pd.Timedelta(minutes=interval)\n",
        "\n",
        "  # rest of code doesn't work if there are no bunches or gaps\n",
        "  # return the empty graph manually\n",
        "  if len(problems) == 0:\n",
        "    # generate list of times according to the interval\n",
        "    start = pd.Timestamp('today').replace(hour=0, minute=0, second=0)\n",
        "    t = start\n",
        "    times = []\n",
        "    while t.day == start.day:\n",
        "      times.append(str(t.time())[:5])\n",
        "      t += interval\n",
        "\n",
        "    return {\n",
        "      \"times\": times,\n",
        "      \"bunches\": [0] * len(times),\n",
        "      \"gaps\": [0] * len(times)\n",
        "    }\n",
        "\n",
        "  # generate the DatetimeIndex needed\n",
        "  index = pd.DatetimeIndex(problems['time'])\n",
        "  df = problems.copy()\n",
        "  df.index = index\n",
        "\n",
        "  # lists for graph data\n",
        "  bunches = []\n",
        "  gaps = []\n",
        "  times = []\n",
        "  \n",
        "  # set selection times\n",
        "  start_date = problems.at[0, 'time'].replace(hour=0, minute=0, second=0)\n",
        "  select_start = start_date\n",
        "  select_end = select_start + interval\n",
        "\n",
        "  while select_start.day == start_date.day:\n",
        "    # get the count of each type of problem in this time interval\n",
        "    count = df.between_time(select_start.time(), select_end.time())['type'].value_counts()\n",
        "\n",
        "    # append the counts to the data list\n",
        "    if 'bunch' in count.index:\n",
        "      bunches.append(int(count['bunch']))\n",
        "    else:\n",
        "      bunches.append(0)\n",
        "    \n",
        "    if 'gap' in count.index:\n",
        "      gaps.append(int(count['gap']))\n",
        "    else:\n",
        "      gaps.append(0)\n",
        "\n",
        "    # save the start time for the x axis\n",
        "    times.append(str(select_start.time())[:5])\n",
        "    \n",
        "    # increment the selection window\n",
        "    select_start += interval\n",
        "    select_end += interval\n",
        "  \n",
        "  return {\n",
        "    \"times\": times,\n",
        "    \"bunches\": bunches,\n",
        "    \"gaps\": gaps\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Toj2AHDL0-8",
        "colab_type": "text"
      },
      "source": [
        "### Generate map data\n",
        "\n",
        "Generates geojson data to show points on a map where all bunches are"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QCQ098hbz7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_simple_geojson(bunches, rid):\n",
        "\n",
        "    geojson = {'type': 'FeatureCollection',\n",
        "               'bunches': create_geojson_features(bunches, rid)}\n",
        "\n",
        "    return geojson\n",
        "\n",
        "def create_geojson_features(df, rid):\n",
        "    \"\"\"\n",
        "    function to generate list of geojson features\n",
        "    for plotting vehicle locations on timestamped map\n",
        "\n",
        "    Expects a dataframe containing lat/lon, vid, timestamp\n",
        "    returns list of basic geojson formatted features:\n",
        "\n",
        "    {\n",
        "      type: Feature\n",
        "      geometry: {\n",
        "        type: Point,\n",
        "        coordinates:[lat, lon]\n",
        "      },\n",
        "      properties: {\n",
        "        time: timestamp\n",
        "        stopId: stop id\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        "    # initializing empty features list\n",
        "    features = []\n",
        "\n",
        "    # iterating through df to pull coords, stopid, timestamp\n",
        "    # and format for json\n",
        "    for index, row in df.iterrows():\n",
        "      feature = {\n",
        "          'type': 'Feature',\n",
        "          'geometry': {\n",
        "              'type':'Point', \n",
        "              'coordinates':[round(row.lon, 4), round(row.lat, 4)]\n",
        "          },\n",
        "          'properties': {\n",
        "              'time': row.time.__str__().rstrip('0').rstrip('.') \n",
        "                      if '.' in row.time.__str__() \n",
        "                      else row.time.__str__(),\n",
        "              'stopId': row.stopId.__str__()\n",
        "          }\n",
        "      }\n",
        "      features.append(feature) # adding point to features list\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHlp7WF8L62j",
        "colab_type": "text"
      },
      "source": [
        "### Calculate overall health\n",
        "\n",
        "Overall health is calculated by averaging the other statistics together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwf5MdngMEpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_health(bunch_percentage, gap_percentage, on_time_percentage):\n",
        "  \"\"\"\n",
        "  Returns an average of 3 main statistics\n",
        "\n",
        "  (does not use coverage since that is already dependent on on-time and bunches)\n",
        "  \"\"\"\n",
        "\n",
        "  # invert bunches and gaps, since fewer of those is better\n",
        "  bunch_percentage = 1 - bunch_percentage\n",
        "  gap_percentage = 1 - gap_percentage\n",
        "\n",
        "  return (bunch_percentage + gap_percentage + on_time_percentage) / 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA6EKTBOvi9a",
        "colab_type": "text"
      },
      "source": [
        "## Run each time to get a new report\n",
        "\n",
        "The next sections generate a report for a single route"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zjzrlBTH1cV",
        "colab_type": "text"
      },
      "source": [
        "### Timing breakdown\n",
        "\n",
        "This section runs each bit individually so we can find which parts of the process take more time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRC8tE0suboI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change this one cell to change to a different route/day\n",
        "# Uses 7am to account for the UTC to PST conversion\n",
        "rid = \"1\"\n",
        "begin = \"2020/6/1 07:00:00\"\n",
        "end = \"2020/6/2 07:00:00\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhc6h6IyxLXO",
        "colab_type": "code",
        "outputId": "1d3bcb7b-3faa-4448-a243-5192c1e84dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# Most time in this cell is waiting on the database responses\n",
        "\n",
        "# Load schedule and route data\n",
        "schedule = Schedule(rid, begin, cnx)\n",
        "route = Route(rid, begin, cnx)\n",
        "\n",
        "# Load bus location data\n",
        "locations = get_location_data(rid, begin, end, cnx)"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 555 ms, sys: 1 ms, total: 556 ms\n",
            "Wall time: 4.74 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuMmUYxm2LsS",
        "colab_type": "code",
        "outputId": "de46e517-ab74-4b6d-8774-117ca7fa11b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# Apply cleaning function (this usually takes 1-2 minutes)\n",
        "locations_old = clean_locations_old(locations, route.stops_table)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 1s, sys: 8.89 ms, total: 1min 1s\n",
            "Wall time: 1min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYdCPj_HjoNj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d3f31ae9-50bb-40e2-d60d-091e23dda17b"
      },
      "source": [
        "%%time\n",
        "# Apply new optimized cleaning function\n",
        "locations = clean_locations(locations, route.stops_table)"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.1 s, sys: 2 ms, total: 3.1 s\n",
            "Wall time: 3.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb3IMAGrjvv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50423493-5655-4c7b-9848-8b19bf3383ad"
      },
      "source": [
        "# Check that all closest stops are assigned the same after optimization\n",
        "print((locations_old['closestStop'] == locations['closestStop']).sum(), \n",
        "      f\"of {len(locations)} stops assigned the same\")"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10445 of 10445 stops assigned the same\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyjqLfb06d56",
        "colab_type": "code",
        "outputId": "03f5d732-a028-4917-97f3-fc831be3523e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# Calculate all times a bus was at each stop\n",
        "stop_times = get_stop_times(locations, route)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.43 s, sys: 998 µs, total: 2.43 s\n",
            "Wall time: 2.43 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozumixf4H4Ut",
        "colab_type": "code",
        "outputId": "961d6330-1ad2-4f62-d11f-58cecedfc2ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# Find all bunches and gaps\n",
        "problems = get_bunches_gaps(stop_times, schedule)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.28 s, sys: 8.01 ms, total: 2.29 s\n",
            "Wall time: 2.28 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIP_e2tOH5lL",
        "colab_type": "code",
        "outputId": "0d41fb89-33e8-4310-b856-57375c230536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# Calculate on-time percentage\n",
        "on_time, total_scheduled = calculate_ontime(stop_times, schedule)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 203 ms, sys: 3 ms, total: 206 ms\n",
            "Wall time: 207 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3q1YHzXTzZr",
        "colab_type": "code",
        "outputId": "ef5c0087-4fdb-4cd2-ac88-c72b25d81ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# Get the bunch/gap graph\n",
        "bg_graph = bunch_gap_graph(problems, interval=10)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 216 ms, sys: 2 ms, total: 218 ms\n",
            "Wall time: 218 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhoseRWVQw91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f9cc036a-8192-4bac-8e61-7d1278f92157"
      },
      "source": [
        "%%time\n",
        "# Get overall health\n",
        "count_times = 0\n",
        "for key in stop_times.keys():\n",
        "  count_times += len(stop_times[key])\n",
        "\n",
        "intervals = count_times-len(stop_times)\n",
        "bunches = len(problems[problems['type'] == 'bunch'])\n",
        "gaps = len(problems[problems['type'] == 'gap'])\n",
        "\n",
        "health = calculate_health(bunches/intervals, gaps/intervals, on_time)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.14 ms, sys: 0 ns, total: 3.14 ms\n",
            "Wall time: 3.4 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPag4nlCb9A5",
        "colab_type": "code",
        "outputId": "055ee9ec-1b6a-491a-91b4-7f6df2cf1ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# Generate the geojson object\n",
        "bunch_df = problems[problems.type.eq('bunch')]\n",
        "bunch_df = bunch_df.merge(route.stops_table, left_on='stop', right_on='tag', how='left')\n",
        "\n",
        "# Creating GeoJSON of bunch times / locations\n",
        "geojson = create_simple_geojson(bunch_df, rid)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 101 ms, sys: 0 ns, total: 101 ms\n",
            "Wall time: 101 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anmmFxMnGbxj",
        "colab_type": "code",
        "outputId": "9cd1c297-0241-499c-929e-c68088bde1b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Print results\n",
        "print(f\"--- Report for route {rid} on {str(pd.to_datetime(begin).date())} ---\")\n",
        "\n",
        "print(f\"\\nOverall health: {health*100 :.2f}%\")\n",
        "\n",
        "print(f\"\\nOut of {intervals} recorded intervals, we found {bunches} bunches and {gaps} gaps\")\n",
        "print(f\"\\t{(bunches/intervals)*100 :.2f}% bunched\")\n",
        "print(f\"\\t{(gaps/intervals)*100 :.2f}% gapped\")\n",
        "\n",
        "print(f\"\\nFound {int(on_time * total_scheduled + .5)} on-time stops out of {total_scheduled} scheduled\")\n",
        "print(f\"On-time percentage is {(on_time)*100 :.2f}%\")\n",
        "\n",
        "coverage = (total_scheduled * on_time + bunches) / total_scheduled\n",
        "print(f\"\\nCoverage was {coverage*100 :.2f}%\")"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Report for route 1 on 2020-06-01 ---\n",
            "\n",
            "Overall health: 73.28%\n",
            "\n",
            "Out of 11385 recorded intervals, we found 532 bunches and 1873 gaps\n",
            "\t4.67% bunched\n",
            "\t16.45% gapped\n",
            "\n",
            "Found 952 on-time stops out of 2324 scheduled\n",
            "On-time percentage is 40.96%\n",
            "\n",
            "Coverage was 63.86%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuDqm1avLb8r",
        "colab_type": "text"
      },
      "source": [
        "### Generating report JSON\n",
        "\n",
        "This function generates the report for a single route and outputs a Python dict, which can be directly converted to JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T35JXa91LfFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_report(rid, date):\n",
        "  \"\"\"\n",
        "  Generates a daily report for the given rid and date\n",
        "\n",
        "  rid : (str)\n",
        "    the route id to generate a report for\n",
        "  \n",
        "  date : (str or pd.Datetime)\n",
        "    the date to generate a report for\n",
        "\n",
        "  returns a dict of the report info\n",
        "  \"\"\"\n",
        "\n",
        "  # get begin and end timestamps for the date\n",
        "  begin = pd.to_datetime(date).replace(hour=7)\n",
        "  end = begin + pd.Timedelta(days=1)\n",
        "  # Load schedule and route data\n",
        "  schedule = Schedule(rid, begin, cnx)\n",
        "  \n",
        "  route = Route(rid, begin, cnx)\n",
        "  \n",
        "  # Load bus location data\n",
        "  locations = get_location_data(rid, begin, end, cnx)\n",
        "  \n",
        "  # Apply cleaning function (this usually takes 1-2 minutes)\n",
        "  locations = clean_locations(locations, route.stops_table)\n",
        "  \n",
        "  # Calculate all times a bus was at each stop\n",
        "  stop_times = get_stop_times(locations, route)\n",
        "\n",
        "  # Find all bunches and gaps\n",
        "  problems = get_bunches_gaps(stop_times, schedule)\n",
        "\n",
        "  # Calculate on-time percentage\n",
        "  on_time, total_scheduled = calculate_ontime(stop_times, schedule)\n",
        "\n",
        "  # Build result dict\n",
        "\n",
        "  # Number of recorded intervals ( sum(len(each list of time)) - number or lists of times)\n",
        "  count_times = 0\n",
        "  for key in stop_times.keys():\n",
        "    count_times += len(stop_times[key])\n",
        "  intervals = count_times-len(stop_times)\n",
        "\n",
        "  # Bunches, gaps, and coverage stats\n",
        "  bunches = len(problems[problems['type'] == 'bunch'])\n",
        "  gaps = len(problems[problems['type'] == 'gap'])\n",
        "  coverage = (total_scheduled * on_time + bunches) / total_scheduled\n",
        "\n",
        "  # Get overall health\n",
        "  health = calculate_health(bunches/intervals, gaps/intervals, on_time)\n",
        "\n",
        "  # Isolating bunches, merging with stops to assign locations to bunches\n",
        "  bunch_df = problems[problems.type.eq('bunch')]\n",
        "  bunch_df = bunch_df.merge(route.stops_table, left_on='stop', right_on='tag', how='left')\n",
        "\n",
        "  # Creating GeoJSON of bunch times / locations\n",
        "  geojson = create_simple_geojson(bunch_df, rid)\n",
        "  \n",
        "  # int/float conversions are because the json library doesn't work with numpy types\n",
        "  result = {\n",
        "      'route_id': rid,\n",
        "      'route_name': route.route_name,\n",
        "      'route_type': route.route_type,\n",
        "      'date': str(pd.to_datetime(date)),\n",
        "      'overall_health': float(round(health * 100, 2)),\n",
        "      'num_bunches': bunches,\n",
        "      'num_gaps': gaps,\n",
        "      'bunched_percentage': round(bunches/intervals*100, 2),\n",
        "      'gapped_percengage': round(gaps/intervals*100, 2),\n",
        "      'total_intervals': intervals,\n",
        "      'on_time_percentage': float(round(on_time * 100, 2)),\n",
        "      'scheduled_stops': int(total_scheduled),\n",
        "      'coverage': float(round(coverage * 100, 2)),\n",
        "      # line_chart contains all data needed to generate the line chart\n",
        "      'line_chart': bunch_gap_graph(problems, interval=10),\n",
        "      # route_table is an array of all rows that should show up in the table\n",
        "      # it will be filled in after all reports are generated\n",
        "      'route_table': [\n",
        "          {\n",
        "            'route_id': rid,\n",
        "            'route_name': route.route_name,\n",
        "            'bunches': bunches,\n",
        "            'gaps': gaps,\n",
        "            'on-time': float(round(on_time * 100, 2)),\n",
        "            'coverage': float(round(coverage * 100, 2))\n",
        "          }\n",
        "      ],\n",
        "      'map_data': geojson\n",
        "  }\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDuD8zIiM1IL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "44d439e3-67dd-4867-e072-ad0a2b2461cf"
      },
      "source": [
        "%%time\n",
        "# Route 1 usage example (before optimization)\n",
        "report_1 = generate_report(rid='1', date='2020/6/1')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 7s, sys: 18 ms, total: 1min 8s\n",
            "Wall time: 1min 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHijm7go048l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "184fd580-2c60-4ef1-ebaa-379f182c4b88"
      },
      "source": [
        "%%time\n",
        "# Route 1 usage example (after optimization)\n",
        "report_1 = generate_report(rid='1', date='2020/6/1')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dropped 0 rows that were .5km or further for a stop\n",
            "CPU times: user 8.98 s, sys: 13.7 ms, total: 8.99 s\n",
            "Wall time: 13 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F0hL92BYZrw",
        "colab_type": "code",
        "outputId": "8f931f30-f2f8-47df-c7c1-ae0a0982da65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "report_1.keys()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['route_id', 'route_name', 'route_type', 'date', 'overall_health', 'num_bunches', 'num_gaps', 'total_intervals', 'on_time_percentage', 'scheduled_stops', 'coverage', 'line_chart', 'route_table', 'map_data'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiHuF_eu08UT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2ddd3a33-6636-4168-ea34-4706fd0f9200"
      },
      "source": [
        "%%time\n",
        "# Route 714 usage example\n",
        "report_714 = generate_report(rid='714', date='2020/6/1')"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 121 ms, sys: 0 ns, total: 121 ms\n",
            "Wall time: 1.07 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoOyHgnEY3oP",
        "colab_type": "code",
        "outputId": "48c93cf1-7941-4917-ccdd-4235e7321b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "r = report_714\n",
        "\n",
        "print('Route:', r['route_name'])\n",
        "print('Health:', r['overall_health'])\n",
        "print('Bunches:', r['num_bunches'])\n",
        "print('Gaps:', r['num_gaps'])\n",
        "print('On-time:', r['on_time_percentage'])\n",
        "print('Coverage:', r['coverage'])"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Route: 714-Bart Early Bird\n",
            "Health: 85.19\n",
            "Bunches: 0\n",
            "Gaps: 0\n",
            "On-time: 55.56\n",
            "Coverage: 55.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I51BkjZQcMAW",
        "colab_type": "text"
      },
      "source": [
        "## Generating report for an entire day\n",
        "\n",
        "This section uses the previous one to generate reports for all routes at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHvTg7PecoBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_active_routes(date):\n",
        "  \"\"\"\n",
        "  returns a list of all route id's that had reported \n",
        "  bus locations on the given date\n",
        "  \"\"\"\n",
        "\n",
        "  # Set hour to 7 to account for UTC time change\n",
        "  date = pd.to_datetime(date).replace(hour=7)\n",
        "  end = date + pd.Timedelta(days=1)\n",
        "\n",
        "  query = \"\"\"\n",
        "    SELECT DISTINCT rid\n",
        "    FROM locations\n",
        "    WHERE timestamp >= %s ::TIMESTAMP AND\n",
        "          timestamp < %s ::TIMESTAMP;\n",
        "  \"\"\"\n",
        "\n",
        "  cursor.execute(query, (date, end))\n",
        "  return [result[0] for result in cursor.fetchall()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW_0FZS9cLWS",
        "colab_type": "code",
        "outputId": "aa46e00f-3f54-4576-a095-3cfbecc2788a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "%%time\n",
        "# This cell typically takes about 3 minutes to run with 28 active routes\n",
        "# (our original un-optimized solution took 20 minutes)\n",
        "\n",
        "# choose a day\n",
        "date = '2020-5-25'\n",
        "\n",
        "# get all active routes \n",
        "route_ids = get_active_routes(date)\n",
        "route_ids.sort()\n",
        "\n",
        "# get the report for all routes\n",
        "all_reports = []\n",
        "for rid in route_ids:\n",
        "  try:\n",
        "    print(f\"Generating report for route {rid}...\")\n",
        "    all_reports.append(generate_report(rid, date))\n",
        "  except KeyboardInterrupt:\n",
        "    # if a user wants to stop this early\n",
        "    break\n",
        "  except Exception as err: \n",
        "    # if any particular route throws an error\n",
        "    print(f\"Route {rid} failed, error:\")\n",
        "    print(err, '\\n')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated report for route 1\n",
            "Generated report for route 12\n",
            "Generated report for route 14\n",
            "Generated report for route 14R\n",
            "Generated report for route 19\n",
            "Generated report for route 22\n",
            "Generated report for route 24\n",
            "Generated report for route 25\n",
            "Generated report for route 29\n",
            "Generated report for route 38\n",
            "Generated report for route 38R\n",
            "Generated report for route 44\n",
            "Generated report for route 49\n",
            "Generated report for route 5\n",
            "Generated report for route 8\n",
            "Generated report for route 9\n",
            "Generated report for route 90\n",
            "Generated report for route 91\n",
            "Generated report for route LBUS\n",
            "Generated report for route L_OWL\n",
            "Generated report for route NBUS\n",
            "Generated report for route N_OWL\n",
            "Generated report for route TBUS\n",
            "CPU times: user 2min 37s, sys: 198 ms, total: 2min 37s\n",
            "Wall time: 3min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeoOjE4HlDAh",
        "colab_type": "code",
        "outputId": "829254e5-c97c-48bc-e379-a06b6b1cc1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_reports)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e69gtrmh0cBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate aggregate reports\n",
        "\n",
        "# read existing reports into a dataframe to work with them easily\n",
        "df = pd.DataFrame(all_reports)\n",
        "\n",
        "# for each aggregate type\n",
        "types =  list(df['route_type'].unique()) + ['All']\n",
        "for t in types:\n",
        "  # filter df to the routes we are adding up\n",
        "  if t == 'All':\n",
        "    filtered = df\n",
        "  else:\n",
        "    filtered = df[df['route_type'] == t]\n",
        "\n",
        "  # on-time percentage: sum([all on-time stops]) / sum([all scheduled stops])\n",
        "  count_on_time = (filtered['on_time_percentage'] * filtered['scheduled_stops']).sum()\n",
        "  on_time_perc = count_on_time / filtered['scheduled_stops'].sum()\n",
        "\n",
        "  # coverage: (sum([all on-time stops]) + sum([all bunches])) / sum([all scheduled stops])\n",
        "  coverage = (count_on_time + filtered['num_bunches'].sum()) / filtered['scheduled_stops'].sum()\n",
        "\n",
        "  # aggregate the graph object\n",
        "  # x-axis is same for all\n",
        "  first = filtered.index[0]\n",
        "  times = filtered.at[first, 'line_chart']['times']\n",
        "\n",
        "  # sum up all y-axis values\n",
        "  bunches = pd.Series(filtered.at[first, 'line_chart']['bunches'])\n",
        "  gaps = pd.Series(filtered.at[first, 'line_chart']['gaps'])\n",
        "\n",
        "  # Get overall health\n",
        "  health = calculate_health(filtered['num_bunches'].sum()/filtered['total_intervals'].sum(), \n",
        "                            filtered['num_gaps'].sum()/filtered['total_intervals'].sum(), \n",
        "                            on_time_perc / 100)\n",
        "\n",
        "  # same pattern for the geojson list\n",
        "  geojson = filtered.at[first, 'map_data']['bunches']\n",
        "\n",
        "  for i, report in filtered[1:].iterrows():\n",
        "    # pd.Series adds all values in the lists together\n",
        "    bunches += pd.Series(report['line_chart']['bunches'])\n",
        "    gaps += pd.Series(report['line_chart']['gaps'])\n",
        "\n",
        "    # lists concatenate together\n",
        "    geojson += report['map_data']['bunches']\n",
        "  \n",
        "  # again, convert to native python types since json doesn't work with numpy types\n",
        "  n_bunches = int(filtered['num_bunches'].sum())\n",
        "  n_gaps = int(filtered['num_gaps'].sum())\n",
        "  n_intervals = int(filtered['total_intervals'].sum())\n",
        "\n",
        "  # save a new report object\n",
        "  new_report = {\n",
        "      'route_id': t,\n",
        "      'route_name': t,\n",
        "      'route_type': t,\n",
        "      'date': all_reports[0]['date'],\n",
        "      'overall_health': float(round(health*100, 2)),\n",
        "      'num_bunches': n_bunches,\n",
        "      'num_gaps': n_gaps,\n",
        "      'bunched_percentage': round(n_bunches/n_intervals*100, 2),\n",
        "      'gapped_percentage': round(n_gaps/n_intervals*100, 2),\n",
        "      'total_intervals': n_intervals,\n",
        "      'on_time_percentage': float(round(on_time_perc, 2)),\n",
        "      'scheduled_stops': int(filtered['scheduled_stops'].sum()),\n",
        "      'coverage': float(round(coverage, 2)),\n",
        "      'line_chart': {\n",
        "          'times': times,\n",
        "          'bunches': list(bunches),\n",
        "          'gaps': list(gaps)\n",
        "      },\n",
        "      'route_table': [\n",
        "          {\n",
        "            'route_id': t,\n",
        "            'route_name': t,\n",
        "            'bunches': n_bunches,\n",
        "            'gaps': n_gaps,\n",
        "            'on-time': float(round(on_time_perc, 2)),\n",
        "            'coverage': float(round(coverage, 2))\n",
        "          }\n",
        "      ],\n",
        "      'map_data': {\n",
        "          'type': 'FeatureCollection',\n",
        "          'bunches': geojson\n",
        "      }\n",
        "  }\n",
        "  \n",
        "  # put aggregate reports at the beginning of the list\n",
        "  all_reports.insert(0, new_report)\n",
        "\n",
        "# Add route_table rows to the aggregate report\n",
        "# Set up a dict to hold each aggregate table\n",
        "tables = {}\n",
        "for t in types:\n",
        "  tables[t] = []\n",
        "\n",
        "# Add rows from each report\n",
        "for report in all_reports:\n",
        "  # add to the route type's table\n",
        "  tables[report['route_type']].append(report['route_table'][0])\n",
        "\n",
        "  # also add to all routes table\n",
        "  if report['route_id'] != \"All\":\n",
        "    # if statement needed to not duplicate the \"All\" row twice\n",
        "    tables['All'].append(report['route_table'][0])\n",
        "\n",
        "\n",
        "# find matching report and set the table there\n",
        "for key in tables.keys():\n",
        "  for report in all_reports:\n",
        "    if report['route_id'] == key:\n",
        "      # override it because the new table includes the row that was already there\n",
        "      report['route_table'] = tables[key]\n",
        "      break  # only 1 report needs each aggregate table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sFAns_VF7MG",
        "colab_type": "code",
        "outputId": "3c8d2481-885f-415d-d65b-c1c99a5aa4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_reports)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Mpb65xBgKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "caa7cd79-b355-43f2-85e1-aaed7a0f8099"
      },
      "source": [
        "# Overall health of all routes\n",
        "all_reports[0]['overall_health']"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67.67"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTT1G45yGix5",
        "colab_type": "text"
      },
      "source": [
        "## Saving the report\n",
        "\n",
        "These two cells allow you to save the report to a file, or directly upload it to our database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxn0dYVCum5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the all_reports object to a file\n",
        "\n",
        "with open(f'report_{date}.json', 'w') as outfile:\n",
        "  json.dump(all_reports, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS17agC_Z-Nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save new report in the database (all one row)\n",
        "\n",
        "query = \"\"\"\n",
        "  INSERT INTO reports (date, report)\n",
        "  VALUES (%s, %s);\n",
        "\"\"\"\n",
        "cursor.execute(query, (date, json.dumps(all_reports)))\n",
        "cnx.commit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJdqXeclwpxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update an existing report in the database\n",
        "\n",
        "query = \"\"\"\n",
        "  UPDATE reports\n",
        "  SET report = %s\n",
        "  WHERE date = %s ::TIMESTAMP;\n",
        "\"\"\"\n",
        "\n",
        "d = str(pd.to_datetime(date).date())\n",
        "cursor.execute(query, (json.dumps(all_reports), d))\n",
        "cnx.commit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVGWgl5aUwEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save new report in the database (separate rows method)\n",
        "\n",
        "# # set up an iterator to go through all the reports\n",
        "# iter_reports = ({\n",
        "#     'date': date,\n",
        "#     'rid': report['route_id'],\n",
        "#     'report': json.dumps(report)\n",
        "# } for report in all_reports)\n",
        "\n",
        "# # build query and insert all rows\n",
        "# query = \"\"\"\n",
        "#   INSERT INTO reports (date, rid, report)\n",
        "#   VALUES (%(date)s, %(rid)s, %(report)s);\n",
        "# \"\"\"\n",
        "# execute_batch(cursor, query, iter_reports)\n",
        "# cnx.commit()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}