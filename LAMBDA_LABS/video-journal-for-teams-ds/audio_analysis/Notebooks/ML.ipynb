{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Python-3-6/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import random\n",
    "import json\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import  History, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12162, 3) \n",
      "\n",
      "negative    7692\n",
      "positive    2575\n",
      "neutral     1895\n",
      "Name: labels, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>SAVEE/JK_sa01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>SAVEE/JK_sa15.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>SAVEE/DC_n13.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>SAVEE/DC_su09.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>SAVEE/DC_n07.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels source               path\n",
       "0  negative  SAVEE  SAVEE/JK_sa01.wav\n",
       "1  negative  SAVEE  SAVEE/JK_sa15.wav\n",
       "2   neutral  SAVEE   SAVEE/DC_n13.wav\n",
       "3  positive  SAVEE  SAVEE/DC_su09.wav\n",
       "4   neutral  SAVEE   SAVEE/DC_n07.wav"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data_path.csv\")\n",
    "print(df.shape, '\\n')\n",
    "print(df.labels.value_counts(), '\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('labels', data=df)\n",
    "plt.title('Balanced Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shuffle the Dataset\n",
    "shuffled_df = df.sample(frac=1,random_state=4)\n",
    "\n",
    "# Put all the positive labels in a separate datasets\n",
    "positive_df = shuffled_df.loc[shuffled_df['labels'] == 'positive']\n",
    "neutral_df = shuffled_df.loc[shuffled_df['labels'] == 'neutral']\n",
    "\n",
    "# Randomly select 2575 observations from the negative label (majority class)\n",
    "negative_df = shuffled_df.loc[shuffled_df['labels'] == 'negative'].sample(n=2575,random_state=42)\n",
    "\n",
    "# Concatenate all three dataframes again\n",
    "normalized_df = pd.concat([negative_df, positive_df, neutral_df])\n",
    "\n",
    "#plot the dataset after the undersampling\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('labels', data=normalized_df)\n",
    "plt.title('Balanced Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7045, 3)\n"
     ]
    }
   ],
   "source": [
    "print(normalized_df.shape)\n",
    "#normalized_df.to_csv(\"NormData_path.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the features of audio files using librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7045/7045 [05:19<00:00, 22.04it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"NormData_path.csv\")\n",
    "data = pd.DataFrame(columns=['feature'])\n",
    "\n",
    "input_duration = 4\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    X, sample_rate = librosa.load(df.path[i],\n",
    "                                  res_type='kaiser_fast',\n",
    "                                  duration=input_duration,\n",
    "                                  sr=22050*2,\n",
    "                                  offset=0.5)\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "    feature = mfccs\n",
    "    data.loc[i] = [feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-17.498186, -16.923052, -18.209496, -19.01889...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-17.277065, -16.79272, -16.40998, -15.706557,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-52.04405, -52.04405, -52.04405, -52.04405, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-21.012785, -17.835165, -17.089836, -18.42785...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-21.351511, -19.535324, -19.565594, -20.22439...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-17.498186, -16.923052, -18.209496, -19.01889...\n",
       "1  [-17.277065, -16.79272, -16.40998, -15.706557,...\n",
       "2  [-52.04405, -52.04405, -52.04405, -52.04405, -...\n",
       "3  [-21.012785, -17.835165, -17.089836, -18.42785...\n",
       "4  [-21.351511, -19.535324, -19.565594, -20.22439..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(data['feature'].values.tolist())\n",
    "labels = df.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.498186</td>\n",
       "      <td>-16.923052</td>\n",
       "      <td>-18.209496</td>\n",
       "      <td>-19.018898</td>\n",
       "      <td>-15.107430</td>\n",
       "      <td>-15.195522</td>\n",
       "      <td>-14.367937</td>\n",
       "      <td>-13.934706</td>\n",
       "      <td>-12.468779</td>\n",
       "      <td>-11.660690</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.277065</td>\n",
       "      <td>-16.792721</td>\n",
       "      <td>-16.409981</td>\n",
       "      <td>-15.706557</td>\n",
       "      <td>-16.806917</td>\n",
       "      <td>-16.667513</td>\n",
       "      <td>-17.196861</td>\n",
       "      <td>-19.235281</td>\n",
       "      <td>-18.453226</td>\n",
       "      <td>-19.312126</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-21.012785</td>\n",
       "      <td>-17.835165</td>\n",
       "      <td>-17.089836</td>\n",
       "      <td>-18.427853</td>\n",
       "      <td>-17.305687</td>\n",
       "      <td>-15.768848</td>\n",
       "      <td>-15.960073</td>\n",
       "      <td>-15.640894</td>\n",
       "      <td>-15.231248</td>\n",
       "      <td>-17.141172</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-21.351511</td>\n",
       "      <td>-19.535324</td>\n",
       "      <td>-19.565594</td>\n",
       "      <td>-20.224398</td>\n",
       "      <td>-19.516762</td>\n",
       "      <td>-18.878918</td>\n",
       "      <td>-19.025806</td>\n",
       "      <td>-18.829952</td>\n",
       "      <td>-19.683552</td>\n",
       "      <td>-18.385675</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 345 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0 -17.498186 -16.923052 -18.209496 -19.018898 -15.107430 -15.195522   \n",
       "1 -17.277065 -16.792721 -16.409981 -15.706557 -16.806917 -16.667513   \n",
       "2 -52.044048 -52.044048 -52.044048 -52.044048 -52.044048 -52.044048   \n",
       "3 -21.012785 -17.835165 -17.089836 -18.427853 -17.305687 -15.768848   \n",
       "4 -21.351511 -19.535324 -19.565594 -20.224398 -19.516762 -18.878918   \n",
       "\n",
       "         6          7          8          9    ...  335  336  337  338  339  \\\n",
       "0 -14.367937 -13.934706 -12.468779 -11.660690  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "1 -17.196861 -19.235281 -18.453226 -19.312126  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "2 -52.044048 -52.044048 -52.044048 -52.044048  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "3 -15.960073 -15.640894 -15.231248 -17.141172  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "4 -19.025806 -18.829952 -19.683552 -18.385675  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   340  341  342  343  344  \n",
       "0  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 345 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.concat([df_new,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7045"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf = newdf.rename(index=str, columns={\"0\": \"label\"})\n",
    "len(rnewdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.498186</td>\n",
       "      <td>-16.923052</td>\n",
       "      <td>-18.209496</td>\n",
       "      <td>-19.018898</td>\n",
       "      <td>-15.107430</td>\n",
       "      <td>-15.195522</td>\n",
       "      <td>-14.367937</td>\n",
       "      <td>-13.934706</td>\n",
       "      <td>-12.468779</td>\n",
       "      <td>-11.660690</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.277065</td>\n",
       "      <td>-16.792721</td>\n",
       "      <td>-16.409981</td>\n",
       "      <td>-15.706557</td>\n",
       "      <td>-16.806917</td>\n",
       "      <td>-16.667513</td>\n",
       "      <td>-17.196861</td>\n",
       "      <td>-19.235281</td>\n",
       "      <td>-18.453226</td>\n",
       "      <td>-19.312126</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-21.012785</td>\n",
       "      <td>-17.835165</td>\n",
       "      <td>-17.089836</td>\n",
       "      <td>-18.427853</td>\n",
       "      <td>-17.305687</td>\n",
       "      <td>-15.768848</td>\n",
       "      <td>-15.960073</td>\n",
       "      <td>-15.640894</td>\n",
       "      <td>-15.231248</td>\n",
       "      <td>-17.141172</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-21.351511</td>\n",
       "      <td>-19.535324</td>\n",
       "      <td>-19.565594</td>\n",
       "      <td>-20.224398</td>\n",
       "      <td>-19.516762</td>\n",
       "      <td>-18.878918</td>\n",
       "      <td>-19.025806</td>\n",
       "      <td>-18.829952</td>\n",
       "      <td>-19.683552</td>\n",
       "      <td>-18.385675</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-7.807920</td>\n",
       "      <td>-8.538022</td>\n",
       "      <td>-8.726308</td>\n",
       "      <td>-10.104364</td>\n",
       "      <td>-9.147784</td>\n",
       "      <td>-9.806588</td>\n",
       "      <td>-9.739620</td>\n",
       "      <td>-9.050481</td>\n",
       "      <td>-9.680884</td>\n",
       "      <td>-10.021114</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-18.958271</td>\n",
       "      <td>-21.169525</td>\n",
       "      <td>-28.681906</td>\n",
       "      <td>-29.814848</td>\n",
       "      <td>-31.028938</td>\n",
       "      <td>-31.090773</td>\n",
       "      <td>-29.703545</td>\n",
       "      <td>-29.393749</td>\n",
       "      <td>-28.719627</td>\n",
       "      <td>-28.598173</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-27.650753</td>\n",
       "      <td>-26.875374</td>\n",
       "      <td>-24.142618</td>\n",
       "      <td>-23.740669</td>\n",
       "      <td>-25.067106</td>\n",
       "      <td>-23.321156</td>\n",
       "      <td>-24.448769</td>\n",
       "      <td>-25.232138</td>\n",
       "      <td>-24.746489</td>\n",
       "      <td>-24.286806</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-6.285365</td>\n",
       "      <td>-10.161021</td>\n",
       "      <td>-27.820263</td>\n",
       "      <td>-26.720375</td>\n",
       "      <td>-24.379763</td>\n",
       "      <td>-23.397623</td>\n",
       "      <td>-24.339931</td>\n",
       "      <td>-25.863529</td>\n",
       "      <td>-23.922239</td>\n",
       "      <td>-24.495483</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-40.779808</td>\n",
       "      <td>-41.069378</td>\n",
       "      <td>-43.116722</td>\n",
       "      <td>-45.314552</td>\n",
       "      <td>-45.559837</td>\n",
       "      <td>-45.141392</td>\n",
       "      <td>-45.566971</td>\n",
       "      <td>-45.866497</td>\n",
       "      <td>-45.585758</td>\n",
       "      <td>-44.924397</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 346 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0 -17.498186 -16.923052 -18.209496 -19.018898 -15.107430 -15.195522   \n",
       "1 -17.277065 -16.792721 -16.409981 -15.706557 -16.806917 -16.667513   \n",
       "2 -52.044048 -52.044048 -52.044048 -52.044048 -52.044048 -52.044048   \n",
       "3 -21.012785 -17.835165 -17.089836 -18.427853 -17.305687 -15.768848   \n",
       "4 -21.351511 -19.535324 -19.565594 -20.224398 -19.516762 -18.878918   \n",
       "5  -7.807920  -8.538022  -8.726308 -10.104364  -9.147784  -9.806588   \n",
       "6 -18.958271 -21.169525 -28.681906 -29.814848 -31.028938 -31.090773   \n",
       "7 -27.650753 -26.875374 -24.142618 -23.740669 -25.067106 -23.321156   \n",
       "8  -6.285365 -10.161021 -27.820263 -26.720375 -24.379763 -23.397623   \n",
       "9 -40.779808 -41.069378 -43.116722 -45.314552 -45.559837 -45.141392   \n",
       "\n",
       "           6          7          8          9  ...  336  337  338  339  340  \\\n",
       "0 -14.367937 -13.934706 -12.468779 -11.660690  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "1 -17.196861 -19.235281 -18.453226 -19.312126  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "2 -52.044048 -52.044048 -52.044048 -52.044048  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "3 -15.960073 -15.640894 -15.231248 -17.141172  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "4 -19.025806 -18.829952 -19.683552 -18.385675  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "5  -9.739620  -9.050481  -9.680884 -10.021114  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "6 -29.703545 -29.393749 -28.719627 -28.598173  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "7 -24.448769 -25.232138 -24.746489 -24.286806  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "8 -24.339931 -25.863529 -23.922239 -24.495483  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "9 -45.566971 -45.866497 -45.585758 -44.924397  ...  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   341  342  343  344    labels  \n",
       "0  NaN  NaN  NaN  NaN  negative  \n",
       "1  NaN  NaN  NaN  NaN  negative  \n",
       "2  NaN  NaN  NaN  NaN  negative  \n",
       "3  NaN  NaN  NaN  NaN  negative  \n",
       "4  NaN  NaN  NaN  NaN  negative  \n",
       "5  NaN  NaN  NaN  NaN  negative  \n",
       "6  NaN  NaN  NaN  NaN  negative  \n",
       "7  NaN  NaN  NaN  NaN  negative  \n",
       "8  NaN  NaN  NaN  NaN  negative  \n",
       "9  NaN  NaN  NaN  NaN  negative  \n",
       "\n",
       "[10 rows x 346 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1169377"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7045, 346)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.498186</td>\n",
       "      <td>-16.923052</td>\n",
       "      <td>-18.209496</td>\n",
       "      <td>-19.018898</td>\n",
       "      <td>-15.107430</td>\n",
       "      <td>-15.195522</td>\n",
       "      <td>-14.367937</td>\n",
       "      <td>-13.934706</td>\n",
       "      <td>-12.468779</td>\n",
       "      <td>-11.660690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.277065</td>\n",
       "      <td>-16.792721</td>\n",
       "      <td>-16.409981</td>\n",
       "      <td>-15.706557</td>\n",
       "      <td>-16.806917</td>\n",
       "      <td>-16.667513</td>\n",
       "      <td>-17.196861</td>\n",
       "      <td>-19.235281</td>\n",
       "      <td>-18.453226</td>\n",
       "      <td>-19.312126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-21.012785</td>\n",
       "      <td>-17.835165</td>\n",
       "      <td>-17.089836</td>\n",
       "      <td>-18.427853</td>\n",
       "      <td>-17.305687</td>\n",
       "      <td>-15.768848</td>\n",
       "      <td>-15.960073</td>\n",
       "      <td>-15.640894</td>\n",
       "      <td>-15.231248</td>\n",
       "      <td>-17.141172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-21.351511</td>\n",
       "      <td>-19.535324</td>\n",
       "      <td>-19.565594</td>\n",
       "      <td>-20.224398</td>\n",
       "      <td>-19.516762</td>\n",
       "      <td>-18.878918</td>\n",
       "      <td>-19.025806</td>\n",
       "      <td>-18.829952</td>\n",
       "      <td>-19.683552</td>\n",
       "      <td>-18.385675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 346 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0 -17.498186 -16.923052 -18.209496 -19.018898 -15.107430 -15.195522   \n",
       "1 -17.277065 -16.792721 -16.409981 -15.706557 -16.806917 -16.667513   \n",
       "2 -52.044048 -52.044048 -52.044048 -52.044048 -52.044048 -52.044048   \n",
       "3 -21.012785 -17.835165 -17.089836 -18.427853 -17.305687 -15.768848   \n",
       "4 -21.351511 -19.535324 -19.565594 -20.224398 -19.516762 -18.878918   \n",
       "\n",
       "           6          7          8          9  ...  336  337  338  339  340  \\\n",
       "0 -14.367937 -13.934706 -12.468779 -11.660690  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "1 -17.196861 -19.235281 -18.453226 -19.312126  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "2 -52.044048 -52.044048 -52.044048 -52.044048  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "3 -15.960073 -15.640894 -15.231248 -17.141172  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "4 -19.025806 -18.829952 -19.683552 -18.385675  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   341  342  343  344    labels  \n",
       "0  0.0  0.0  0.0  0.0  negative  \n",
       "1  0.0  0.0  0.0  0.0  negative  \n",
       "2  0.0  0.0  0.0  0.0  negative  \n",
       "3  0.0  0.0  0.0  0.0  negative  \n",
       "4  0.0  0.0  0.0  0.0  negative  \n",
       "\n",
       "[5 rows x 346 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf = rnewdf.fillna(0)\n",
    "print(rnewdf.shape)\n",
    "rnewdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(data):\n",
    "    \"\"\"\n",
    "    Plot the Audio Frequency.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    plt.title('Raw wave ')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.plot(np.linspace(0, 1, len(data)), data)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def noise(data):\n",
    "    \"\"\"\n",
    "    Adding White Noise.\n",
    "    \"\"\"\n",
    "    # you can take any distribution from https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html\n",
    "    noise_amp = 0.005*np.random.uniform()*np.amax(data)\n",
    "    data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "    \n",
    "def shift(data):\n",
    "    \"\"\"\n",
    "    Random Shifting.\n",
    "    \"\"\"\n",
    "    s_range = int(np.random.uniform(low=-5, high = 5)*500)\n",
    "    return np.roll(data, s_range)\n",
    "    \n",
    "def stretch(data, rate=0.8):\n",
    "    \"\"\"\n",
    "    Streching the Sound.\n",
    "    \"\"\"\n",
    "    data = librosa.effects.time_stretch(data, rate)\n",
    "    return data\n",
    "\n",
    "def pitch(data, sample_rate):\n",
    "    \"\"\"\n",
    "    Pitch Tuning.\n",
    "    \"\"\"\n",
    "    bins_per_octave = 12\n",
    "    pitch_pm = 2\n",
    "    pitch_change =  pitch_pm * 2*(np.random.uniform())   \n",
    "    data = librosa.effects.pitch_shift(data.astype('float64'), \n",
    "                                      sample_rate, n_steps=pitch_change, \n",
    "                                      bins_per_octave=bins_per_octave)\n",
    "    return data\n",
    "    \n",
    "def dyn_change(data):\n",
    "    \"\"\"\n",
    "    Random Value Change.\n",
    "    \"\"\"\n",
    "    dyn_change = np.random.uniform(low=1.5,high=3)\n",
    "    return (data * dyn_change)\n",
    "    \n",
    "def speedNpitch(data):\n",
    "    \"\"\"\n",
    "    Speed and Pitch Tuning.\n",
    "    \"\"\"\n",
    "    # you can change low and high here\n",
    "    length_change = np.random.uniform(low=0.8, high = 1)\n",
    "    speed_fac = 1.0  / length_change\n",
    "    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n",
    "    minlen = min(data.shape[0], tmp.shape[0])\n",
    "    data *= 0\n",
    "    data[0:minlen] = tmp[0:minlen]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7045/7045 [06:18<00:00, 18.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Augmentation Method 1\n",
    "\n",
    "syn_data1 = pd.DataFrame(columns=['feature', 'labels'])\n",
    "for i in tqdm(range(len(df))):\n",
    "    X, sample_rate = librosa.load(df.path[i],\n",
    "                                  res_type='kaiser_fast',\n",
    "                                  duration=input_duration,\n",
    "                                  sr=22050*2,\n",
    "                                  offset=0.5)\n",
    "    if df.labels[i]:\n",
    "        X = noise(X)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "        feature = mfccs\n",
    "        a = random.uniform(0, 1)\n",
    "        syn_data1.loc[i] = [feature, df.labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7045/7045 [30:31<00:00,  3.85it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Augmentation Method 2\n",
    "\n",
    "syn_data2 = pd.DataFrame(columns=['feature', 'labels'])\n",
    "for i in tqdm(range(len(df))):\n",
    "    X, sample_rate = librosa.load(df.path[i],\n",
    "                                  res_type='kaiser_fast',\n",
    "                                  duration=input_duration,\n",
    "                                  sr=22050*2,\n",
    "                                  offset=0.5)\n",
    "    if df.labels[i]:\n",
    "        X = pitch(X, sample_rate)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "        feature = mfccs\n",
    "        a = random.uniform(0, 1)\n",
    "        syn_data2.loc[i] = [feature, df.labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7045, 7045)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(syn_data1), len(syn_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data1 = syn_data1.reset_index(drop=True)\n",
    "syn_data2 = syn_data2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7045"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(syn_data1['feature'].values.tolist())\n",
    "labels2 = syn_data1.labels\n",
    "syndf1 = pd.concat([df2,labels2], axis=1)\n",
    "syndf1 = syndf1.rename(index=str, columns={\"0\": \"label\"})\n",
    "syndf1 = syndf1.fillna(0)\n",
    "len(syndf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.498562</td>\n",
       "      <td>-16.924034</td>\n",
       "      <td>-18.209293</td>\n",
       "      <td>-19.018007</td>\n",
       "      <td>-15.107864</td>\n",
       "      <td>-15.196448</td>\n",
       "      <td>-14.368540</td>\n",
       "      <td>-13.935128</td>\n",
       "      <td>-12.467757</td>\n",
       "      <td>-11.660300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.167120</td>\n",
       "      <td>-16.602028</td>\n",
       "      <td>-16.314634</td>\n",
       "      <td>-15.708340</td>\n",
       "      <td>-16.738106</td>\n",
       "      <td>-16.557055</td>\n",
       "      <td>-17.071869</td>\n",
       "      <td>-19.088345</td>\n",
       "      <td>-18.293791</td>\n",
       "      <td>-19.242479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-45.307632</td>\n",
       "      <td>-43.321396</td>\n",
       "      <td>-42.405796</td>\n",
       "      <td>-45.283878</td>\n",
       "      <td>-45.582024</td>\n",
       "      <td>-45.081367</td>\n",
       "      <td>-44.080818</td>\n",
       "      <td>-43.217430</td>\n",
       "      <td>-44.806641</td>\n",
       "      <td>-46.739632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-19.011932</td>\n",
       "      <td>-16.472893</td>\n",
       "      <td>-15.852385</td>\n",
       "      <td>-17.734648</td>\n",
       "      <td>-16.173740</td>\n",
       "      <td>-14.430904</td>\n",
       "      <td>-15.362841</td>\n",
       "      <td>-15.169073</td>\n",
       "      <td>-14.003381</td>\n",
       "      <td>-15.839488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-20.452738</td>\n",
       "      <td>-18.876011</td>\n",
       "      <td>-19.108355</td>\n",
       "      <td>-19.539534</td>\n",
       "      <td>-18.730267</td>\n",
       "      <td>-18.108244</td>\n",
       "      <td>-18.084749</td>\n",
       "      <td>-18.008375</td>\n",
       "      <td>-18.991880</td>\n",
       "      <td>-17.731855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 346 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0 -17.498562 -16.924034 -18.209293 -19.018007 -15.107864 -15.196448   \n",
       "1 -17.167120 -16.602028 -16.314634 -15.708340 -16.738106 -16.557055   \n",
       "2 -45.307632 -43.321396 -42.405796 -45.283878 -45.582024 -45.081367   \n",
       "3 -19.011932 -16.472893 -15.852385 -17.734648 -16.173740 -14.430904   \n",
       "4 -20.452738 -18.876011 -19.108355 -19.539534 -18.730267 -18.108244   \n",
       "\n",
       "           6          7          8          9  ...  336  337  338  339  340  \\\n",
       "0 -14.368540 -13.935128 -12.467757 -11.660300  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "1 -17.071869 -19.088345 -18.293791 -19.242479  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "2 -44.080818 -43.217430 -44.806641 -46.739632  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "3 -15.362841 -15.169073 -14.003381 -15.839488  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "4 -18.084749 -18.008375 -18.991880 -17.731855  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   341  342  343  344    labels  \n",
       "0  0.0  0.0  0.0  0.0  negative  \n",
       "1  0.0  0.0  0.0  0.0  negative  \n",
       "2  0.0  0.0  0.0  0.0  negative  \n",
       "3  0.0  0.0  0.0  0.0  negative  \n",
       "4  0.0  0.0  0.0  0.0  negative  \n",
       "\n",
       "[5 rows x 346 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syndf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7045"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(syn_data2['feature'].values.tolist())\n",
    "labels2 = syn_data2.labels\n",
    "syndf2 = pd.concat([df2,labels2], axis=1)\n",
    "syndf2 = syndf2.rename(index=str, columns={\"0\": \"label\"})\n",
    "syndf2 = syndf2.fillna(0)\n",
    "len(syndf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-16.606407</td>\n",
       "      <td>-16.918339</td>\n",
       "      <td>-19.548260</td>\n",
       "      <td>-20.429079</td>\n",
       "      <td>-17.483339</td>\n",
       "      <td>-17.244930</td>\n",
       "      <td>-15.826342</td>\n",
       "      <td>-14.681252</td>\n",
       "      <td>-13.182990</td>\n",
       "      <td>-14.463037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-18.425529</td>\n",
       "      <td>-18.081388</td>\n",
       "      <td>-18.641365</td>\n",
       "      <td>-19.709063</td>\n",
       "      <td>-19.458143</td>\n",
       "      <td>-20.010715</td>\n",
       "      <td>-21.139753</td>\n",
       "      <td>-22.736238</td>\n",
       "      <td>-25.335964</td>\n",
       "      <td>-23.896605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>-52.731750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-21.579000</td>\n",
       "      <td>-19.446978</td>\n",
       "      <td>-19.570080</td>\n",
       "      <td>-20.203712</td>\n",
       "      <td>-21.154814</td>\n",
       "      <td>-20.689070</td>\n",
       "      <td>-19.082752</td>\n",
       "      <td>-17.419336</td>\n",
       "      <td>-17.003222</td>\n",
       "      <td>-18.016191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-21.187592</td>\n",
       "      <td>-21.303402</td>\n",
       "      <td>-23.373610</td>\n",
       "      <td>-25.295097</td>\n",
       "      <td>-22.336639</td>\n",
       "      <td>-22.826536</td>\n",
       "      <td>-23.531553</td>\n",
       "      <td>-23.610966</td>\n",
       "      <td>-23.240740</td>\n",
       "      <td>-23.712269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 346 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0 -16.606407 -16.918339 -19.548260 -20.429079 -17.483339 -17.244930   \n",
       "1 -18.425529 -18.081388 -18.641365 -19.709063 -19.458143 -20.010715   \n",
       "2 -52.731750 -52.731750 -52.731750 -52.731750 -52.731750 -52.731750   \n",
       "3 -21.579000 -19.446978 -19.570080 -20.203712 -21.154814 -20.689070   \n",
       "4 -21.187592 -21.303402 -23.373610 -25.295097 -22.336639 -22.826536   \n",
       "\n",
       "           6          7          8          9  ...  336  337  338  339  340  \\\n",
       "0 -15.826342 -14.681252 -13.182990 -14.463037  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "1 -21.139753 -22.736238 -25.335964 -23.896605  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "2 -52.731750 -52.731750 -52.731750 -52.731750  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "3 -19.082752 -17.419336 -17.003222 -18.016191  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "4 -23.531553 -23.610966 -23.240740 -23.712269  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   341  342  343  344    labels  \n",
       "0  0.0  0.0  0.0  0.0  negative  \n",
       "1  0.0  0.0  0.0  0.0  negative  \n",
       "2  0.0  0.0  0.0  0.0  negative  \n",
       "3  0.0  0.0  0.0  0.0  negative  \n",
       "4  0.0  0.0  0.0  0.0  negative  \n",
       "\n",
       "[5 rows x 346 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syndf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.498186</td>\n",
       "      <td>-16.923052</td>\n",
       "      <td>-18.209496</td>\n",
       "      <td>-19.018898</td>\n",
       "      <td>-15.107430</td>\n",
       "      <td>-15.195522</td>\n",
       "      <td>-14.367937</td>\n",
       "      <td>-13.934706</td>\n",
       "      <td>-12.468779</td>\n",
       "      <td>-11.660690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.277065</td>\n",
       "      <td>-16.792721</td>\n",
       "      <td>-16.409981</td>\n",
       "      <td>-15.706557</td>\n",
       "      <td>-16.806917</td>\n",
       "      <td>-16.667513</td>\n",
       "      <td>-17.196861</td>\n",
       "      <td>-19.235281</td>\n",
       "      <td>-18.453226</td>\n",
       "      <td>-19.312126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>-52.044048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-21.012785</td>\n",
       "      <td>-17.835165</td>\n",
       "      <td>-17.089836</td>\n",
       "      <td>-18.427853</td>\n",
       "      <td>-17.305687</td>\n",
       "      <td>-15.768848</td>\n",
       "      <td>-15.960073</td>\n",
       "      <td>-15.640894</td>\n",
       "      <td>-15.231248</td>\n",
       "      <td>-17.141172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-21.351511</td>\n",
       "      <td>-19.535324</td>\n",
       "      <td>-19.565594</td>\n",
       "      <td>-20.224398</td>\n",
       "      <td>-19.516762</td>\n",
       "      <td>-18.878918</td>\n",
       "      <td>-19.025806</td>\n",
       "      <td>-18.829952</td>\n",
       "      <td>-19.683552</td>\n",
       "      <td>-18.385675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 346 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0 -17.498186 -16.923052 -18.209496 -19.018898 -15.107430 -15.195522   \n",
       "1 -17.277065 -16.792721 -16.409981 -15.706557 -16.806917 -16.667513   \n",
       "2 -52.044048 -52.044048 -52.044048 -52.044048 -52.044048 -52.044048   \n",
       "3 -21.012785 -17.835165 -17.089836 -18.427853 -17.305687 -15.768848   \n",
       "4 -21.351511 -19.535324 -19.565594 -20.224398 -19.516762 -18.878918   \n",
       "\n",
       "           6          7          8          9  ...  336  337  338  339  340  \\\n",
       "0 -14.367937 -13.934706 -12.468779 -11.660690  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "1 -17.196861 -19.235281 -18.453226 -19.312126  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "2 -52.044048 -52.044048 -52.044048 -52.044048  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "3 -15.960073 -15.640894 -15.231248 -17.141172  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "4 -19.025806 -18.829952 -19.683552 -18.385675  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   341  342  343  344    labels  \n",
       "0  0.0  0.0  0.0  0.0  negative  \n",
       "1  0.0  0.0  0.0  0.0  negative  \n",
       "2  0.0  0.0  0.0  0.0  negative  \n",
       "3  0.0  0.0  0.0  0.0  negative  \n",
       "4  0.0  0.0  0.0  0.0  negative  \n",
       "\n",
       "[5 rows x 346 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the Augmented data with original\n",
    "combined_df = pd.concat([rnewdf, syndf1, syndf2], ignore_index=True)\n",
    "combined_df = combined_df.fillna(0)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Stratified Shuffle Split\n",
    "\n",
    "X = combined_df.drop(['labels'], axis=1)\n",
    "y = combined_df.labels\n",
    "xxx = StratifiedShuffleSplit(1, test_size=0.2, random_state=12)\n",
    "for train_index, test_index in xxx.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    6180\n",
       "negative    6180\n",
       "neutral     4548\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for NaN values\n",
    "X_train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16908, 345)\n",
      "['negative' 'neutral' 'positive']\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickel the lb object for future use \n",
    "#filename = 'labels'\n",
    "#outfile = open(filename,'wb')\n",
    "#pickle.dump(lb,outfile)\n",
    "#outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing dimension for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Keras util functions\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "#def fscore(y_true, y_pred):\n",
    "#    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "#        return 0\n",
    "#\n",
    "#    p = precision(y_true, y_pred)\n",
    "#    r = recall(y_true, y_pred)\n",
    "#    f_score = 2 * (p * r) / (p + r + K.epsilon())\n",
    "#    return f_score\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(256, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "# Edit according to target class no.\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 345, 256)          2304      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 345, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 345, 256)          524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 345, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 345, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 345, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 43, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 43, 128)           262272    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 43, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 43, 128)           131200    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 43, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 43, 128)           131200    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 43, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 43, 128)           131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 43, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 43, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 43, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5, 64)             65600     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5, 64)             32832     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 963       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,283,651\n",
      "Trainable params: 1,282,883\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Plotting Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16908 samples, validate on 4227 samples\n",
      "Epoch 1/500\n",
      "16908/16908 [==============================] - 1343s 79ms/step - loss: 1.0450 - accuracy: 0.4653 - val_loss: 1.0340 - val_accuracy: 0.4859\n",
      "Epoch 2/500\n",
      "16908/16908 [==============================] - 389s 23ms/step - loss: 1.0023 - accuracy: 0.4969 - val_loss: 1.0114 - val_accuracy: 0.4897\n",
      "Epoch 3/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.9842 - accuracy: 0.5154 - val_loss: 1.0093 - val_accuracy: 0.5044\n",
      "Epoch 4/500\n",
      "16908/16908 [==============================] - 332s 20ms/step - loss: 0.9705 - accuracy: 0.5238 - val_loss: 1.0002 - val_accuracy: 0.5093\n",
      "Epoch 5/500\n",
      "16908/16908 [==============================] - 328s 19ms/step - loss: 0.9598 - accuracy: 0.5310 - val_loss: 0.9943 - val_accuracy: 0.5157\n",
      "Epoch 6/500\n",
      "16908/16908 [==============================] - 334s 20ms/step - loss: 0.9459 - accuracy: 0.5444 - val_loss: 0.9813 - val_accuracy: 0.5304\n",
      "Epoch 7/500\n",
      "16908/16908 [==============================] - 334s 20ms/step - loss: 0.9418 - accuracy: 0.5406 - val_loss: 0.9863 - val_accuracy: 0.5228\n",
      "Epoch 8/500\n",
      "16908/16908 [==============================] - 330s 20ms/step - loss: 0.9335 - accuracy: 0.5467 - val_loss: 0.9656 - val_accuracy: 0.5174\n",
      "Epoch 9/500\n",
      "16908/16908 [==============================] - 336s 20ms/step - loss: 0.9258 - accuracy: 0.5525 - val_loss: 0.9531 - val_accuracy: 0.5533\n",
      "Epoch 10/500\n",
      "16908/16908 [==============================] - 336s 20ms/step - loss: 0.9158 - accuracy: 0.5616 - val_loss: 0.9616 - val_accuracy: 0.5351\n",
      "Epoch 11/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.9100 - accuracy: 0.5642 - val_loss: 0.9478 - val_accuracy: 0.5477\n",
      "Epoch 12/500\n",
      "16908/16908 [==============================] - 333s 20ms/step - loss: 0.9041 - accuracy: 0.5615 - val_loss: 0.9660 - val_accuracy: 0.5297\n",
      "Epoch 13/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.8969 - accuracy: 0.5670 - val_loss: 0.9471 - val_accuracy: 0.5420\n",
      "Epoch 14/500\n",
      "16908/16908 [==============================] - 332s 20ms/step - loss: 0.8909 - accuracy: 0.5722 - val_loss: 0.9379 - val_accuracy: 0.5441\n",
      "Epoch 15/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.8868 - accuracy: 0.5738 - val_loss: 0.9222 - val_accuracy: 0.5645\n",
      "Epoch 16/500\n",
      "16908/16908 [==============================] - 327s 19ms/step - loss: 0.8784 - accuracy: 0.5777 - val_loss: 0.9212 - val_accuracy: 0.5633\n",
      "Epoch 17/500\n",
      "16908/16908 [==============================] - 330s 19ms/step - loss: 0.8774 - accuracy: 0.5777 - val_loss: 0.9193 - val_accuracy: 0.5581\n",
      "Epoch 18/500\n",
      "16908/16908 [==============================] - 332s 20ms/step - loss: 0.8685 - accuracy: 0.5842 - val_loss: 0.9301 - val_accuracy: 0.5415\n",
      "Epoch 19/500\n",
      "16908/16908 [==============================] - 335s 20ms/step - loss: 0.8659 - accuracy: 0.5843 - val_loss: 0.9184 - val_accuracy: 0.5550\n",
      "Epoch 20/500\n",
      "16908/16908 [==============================] - 327s 19ms/step - loss: 0.8619 - accuracy: 0.5881 - val_loss: 0.9343 - val_accuracy: 0.5399\n",
      "Epoch 21/500\n",
      "16908/16908 [==============================] - 331s 20ms/step - loss: 0.8554 - accuracy: 0.5906 - val_loss: 0.9245 - val_accuracy: 0.5375\n",
      "Epoch 22/500\n",
      "16908/16908 [==============================] - 333s 20ms/step - loss: 0.8528 - accuracy: 0.5898 - val_loss: 0.8944 - val_accuracy: 0.5735\n",
      "Epoch 23/500\n",
      "16908/16908 [==============================] - 334s 20ms/step - loss: 0.8449 - accuracy: 0.5972 - val_loss: 0.8925 - val_accuracy: 0.5777\n",
      "Epoch 24/500\n",
      "16908/16908 [==============================] - 330s 20ms/step - loss: 0.8413 - accuracy: 0.5947 - val_loss: 0.8883 - val_accuracy: 0.5780\n",
      "Epoch 25/500\n",
      "16908/16908 [==============================] - 335s 20ms/step - loss: 0.8405 - accuracy: 0.5985 - val_loss: 0.8995 - val_accuracy: 0.5692\n",
      "Epoch 26/500\n",
      "16908/16908 [==============================] - 331s 20ms/step - loss: 0.8379 - accuracy: 0.5972 - val_loss: 0.9059 - val_accuracy: 0.5446\n",
      "Epoch 27/500\n",
      "16908/16908 [==============================] - 332s 20ms/step - loss: 0.8331 - accuracy: 0.6024 - val_loss: 0.9168 - val_accuracy: 0.5358\n",
      "Epoch 28/500\n",
      "16908/16908 [==============================] - 335s 20ms/step - loss: 0.8298 - accuracy: 0.6031 - val_loss: 0.8736 - val_accuracy: 0.5888\n",
      "Epoch 29/500\n",
      "16908/16908 [==============================] - 333s 20ms/step - loss: 0.8304 - accuracy: 0.6012 - val_loss: 0.8884 - val_accuracy: 0.5761\n",
      "Epoch 30/500\n",
      "16908/16908 [==============================] - 332s 20ms/step - loss: 0.8201 - accuracy: 0.6096 - val_loss: 0.8751 - val_accuracy: 0.5824\n",
      "Epoch 31/500\n",
      "16908/16908 [==============================] - 333s 20ms/step - loss: 0.8244 - accuracy: 0.6031 - val_loss: 0.8694 - val_accuracy: 0.5921\n",
      "Epoch 32/500\n",
      "16908/16908 [==============================] - 333s 20ms/step - loss: 0.8154 - accuracy: 0.6118 - val_loss: 0.8641 - val_accuracy: 0.5910\n",
      "Epoch 33/500\n",
      "16908/16908 [==============================] - 334s 20ms/step - loss: 0.8097 - accuracy: 0.6160 - val_loss: 0.8579 - val_accuracy: 0.6044\n",
      "Epoch 34/500\n",
      "16908/16908 [==============================] - 332s 20ms/step - loss: 0.8089 - accuracy: 0.6185 - val_loss: 0.8685 - val_accuracy: 0.5770\n",
      "Epoch 35/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.8050 - accuracy: 0.6170 - val_loss: 0.8792 - val_accuracy: 0.5692\n",
      "Epoch 36/500\n",
      "16908/16908 [==============================] - 324s 19ms/step - loss: 0.8023 - accuracy: 0.6222 - val_loss: 0.8797 - val_accuracy: 0.5664\n",
      "Epoch 37/500\n",
      "16908/16908 [==============================] - 320s 19ms/step - loss: 0.7979 - accuracy: 0.6237 - val_loss: 0.8527 - val_accuracy: 0.6026\n",
      "Epoch 38/500\n",
      "16908/16908 [==============================] - 320s 19ms/step - loss: 0.7965 - accuracy: 0.6256 - val_loss: 0.8623 - val_accuracy: 0.6066\n",
      "Epoch 39/500\n",
      "16908/16908 [==============================] - 317s 19ms/step - loss: 0.7944 - accuracy: 0.6221 - val_loss: 0.8562 - val_accuracy: 0.5985\n",
      "Epoch 40/500\n",
      "16908/16908 [==============================] - 319s 19ms/step - loss: 0.7894 - accuracy: 0.6280 - val_loss: 0.8582 - val_accuracy: 0.5803\n",
      "Epoch 41/500\n",
      "16908/16908 [==============================] - 314s 19ms/step - loss: 0.7903 - accuracy: 0.6257 - val_loss: 0.8330 - val_accuracy: 0.6241\n",
      "Epoch 42/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.7811 - accuracy: 0.6317 - val_loss: 0.8716 - val_accuracy: 0.5765\n",
      "Epoch 43/500\n",
      "16908/16908 [==============================] - 311s 18ms/step - loss: 0.7805 - accuracy: 0.6340 - val_loss: 0.8515 - val_accuracy: 0.5985\n",
      "Epoch 44/500\n",
      "16908/16908 [==============================] - 317s 19ms/step - loss: 0.7741 - accuracy: 0.6357 - val_loss: 0.8344 - val_accuracy: 0.6179\n",
      "Epoch 45/500\n",
      "16908/16908 [==============================] - 315s 19ms/step - loss: 0.7738 - accuracy: 0.6363 - val_loss: 0.8226 - val_accuracy: 0.6248\n",
      "Epoch 46/500\n",
      "16908/16908 [==============================] - 312s 18ms/step - loss: 0.7726 - accuracy: 0.6365 - val_loss: 0.8448 - val_accuracy: 0.5867\n",
      "Epoch 47/500\n",
      "16908/16908 [==============================] - 312s 18ms/step - loss: 0.7741 - accuracy: 0.6361 - val_loss: 0.8316 - val_accuracy: 0.6073\n",
      "Epoch 48/500\n",
      "16908/16908 [==============================] - 314s 19ms/step - loss: 0.7653 - accuracy: 0.6424 - val_loss: 0.8187 - val_accuracy: 0.6229\n",
      "Epoch 49/500\n",
      "16908/16908 [==============================] - 313s 19ms/step - loss: 0.7603 - accuracy: 0.6461 - val_loss: 0.8477 - val_accuracy: 0.5839\n",
      "Epoch 50/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.7624 - accuracy: 0.6443 - val_loss: 0.8252 - val_accuracy: 0.6215\n",
      "Epoch 51/500\n",
      "16908/16908 [==============================] - 308s 18ms/step - loss: 0.7581 - accuracy: 0.6498 - val_loss: 0.8486 - val_accuracy: 0.6054\n",
      "Epoch 52/500\n",
      "16908/16908 [==============================] - 311s 18ms/step - loss: 0.7552 - accuracy: 0.6451 - val_loss: 0.8115 - val_accuracy: 0.6260\n",
      "Epoch 53/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.7512 - accuracy: 0.6493 - val_loss: 0.8165 - val_accuracy: 0.6234\n",
      "Epoch 54/500\n",
      "16908/16908 [==============================] - 307s 18ms/step - loss: 0.7529 - accuracy: 0.6453 - val_loss: 0.8193 - val_accuracy: 0.6279\n",
      "Epoch 55/500\n",
      "16908/16908 [==============================] - 310s 18ms/step - loss: 0.7483 - accuracy: 0.6496 - val_loss: 0.8112 - val_accuracy: 0.6177\n",
      "Epoch 56/500\n",
      "16908/16908 [==============================] - 317s 19ms/step - loss: 0.7433 - accuracy: 0.6498 - val_loss: 0.8258 - val_accuracy: 0.6030\n",
      "Epoch 57/500\n",
      "16908/16908 [==============================] - 310s 18ms/step - loss: 0.7376 - accuracy: 0.6577 - val_loss: 0.8208 - val_accuracy: 0.6092\n",
      "Epoch 58/500\n",
      "16908/16908 [==============================] - 308s 18ms/step - loss: 0.7385 - accuracy: 0.6550 - val_loss: 0.8078 - val_accuracy: 0.6231\n",
      "Epoch 59/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.7373 - accuracy: 0.6576 - val_loss: 0.7931 - val_accuracy: 0.6444\n",
      "Epoch 60/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.7325 - accuracy: 0.6599 - val_loss: 0.7903 - val_accuracy: 0.6480\n",
      "Epoch 61/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.7298 - accuracy: 0.6603 - val_loss: 0.7896 - val_accuracy: 0.6418\n",
      "Epoch 62/500\n",
      "16908/16908 [==============================] - 305s 18ms/step - loss: 0.7292 - accuracy: 0.6642 - val_loss: 0.7869 - val_accuracy: 0.6390\n",
      "Epoch 63/500\n",
      "16908/16908 [==============================] - 306s 18ms/step - loss: 0.7217 - accuracy: 0.6683 - val_loss: 0.7971 - val_accuracy: 0.6354\n",
      "Epoch 64/500\n",
      "16908/16908 [==============================] - 308s 18ms/step - loss: 0.7230 - accuracy: 0.6632 - val_loss: 0.7964 - val_accuracy: 0.6217\n",
      "Epoch 65/500\n",
      "16908/16908 [==============================] - 305s 18ms/step - loss: 0.7195 - accuracy: 0.6656 - val_loss: 0.8131 - val_accuracy: 0.6182\n",
      "Epoch 66/500\n",
      "16908/16908 [==============================] - 304s 18ms/step - loss: 0.7197 - accuracy: 0.6680 - val_loss: 0.8410 - val_accuracy: 0.6054\n",
      "Epoch 67/500\n",
      "16908/16908 [==============================] - 302s 18ms/step - loss: 0.7196 - accuracy: 0.6673 - val_loss: 0.8063 - val_accuracy: 0.6075\n",
      "Epoch 68/500\n",
      "16908/16908 [==============================] - 307s 18ms/step - loss: 0.7135 - accuracy: 0.6728 - val_loss: 0.7965 - val_accuracy: 0.6326\n",
      "Epoch 69/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.7110 - accuracy: 0.6715 - val_loss: 0.8013 - val_accuracy: 0.6248\n",
      "Epoch 70/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.7078 - accuracy: 0.6773 - val_loss: 0.7714 - val_accuracy: 0.6582\n",
      "Epoch 71/500\n",
      "16908/16908 [==============================] - 302s 18ms/step - loss: 0.7035 - accuracy: 0.6764 - val_loss: 0.7584 - val_accuracy: 0.6728\n",
      "Epoch 72/500\n",
      "16908/16908 [==============================] - 302s 18ms/step - loss: 0.7040 - accuracy: 0.6759 - val_loss: 0.7792 - val_accuracy: 0.6373\n",
      "Epoch 73/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.6963 - accuracy: 0.6795 - val_loss: 0.7908 - val_accuracy: 0.6229\n",
      "Epoch 74/500\n",
      "16908/16908 [==============================] - 297s 18ms/step - loss: 0.6941 - accuracy: 0.6816 - val_loss: 0.7771 - val_accuracy: 0.6390\n",
      "Epoch 75/500\n",
      "16908/16908 [==============================] - 296s 17ms/step - loss: 0.6952 - accuracy: 0.6803 - val_loss: 0.7623 - val_accuracy: 0.6664\n",
      "Epoch 76/500\n",
      "16908/16908 [==============================] - 298s 18ms/step - loss: 0.6906 - accuracy: 0.6825 - val_loss: 0.7833 - val_accuracy: 0.6369\n",
      "Epoch 77/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.6903 - accuracy: 0.6865 - val_loss: 0.7712 - val_accuracy: 0.6449\n",
      "Epoch 78/500\n",
      "16908/16908 [==============================] - 298s 18ms/step - loss: 0.6828 - accuracy: 0.6891 - val_loss: 0.7590 - val_accuracy: 0.6662\n",
      "Epoch 79/500\n",
      "16908/16908 [==============================] - 295s 17ms/step - loss: 0.6852 - accuracy: 0.6896 - val_loss: 0.7841 - val_accuracy: 0.6314\n",
      "Epoch 80/500\n",
      "16908/16908 [==============================] - 292s 17ms/step - loss: 0.6805 - accuracy: 0.6914 - val_loss: 0.7707 - val_accuracy: 0.6376\n",
      "Epoch 81/500\n",
      "16908/16908 [==============================] - 296s 18ms/step - loss: 0.6789 - accuracy: 0.6933 - val_loss: 0.7700 - val_accuracy: 0.6359\n",
      "Epoch 82/500\n",
      "16908/16908 [==============================] - 292s 17ms/step - loss: 0.6789 - accuracy: 0.6913 - val_loss: 0.8144 - val_accuracy: 0.6179\n",
      "Epoch 83/500\n",
      "16908/16908 [==============================] - 292s 17ms/step - loss: 0.6774 - accuracy: 0.6908 - val_loss: 0.7601 - val_accuracy: 0.6482\n",
      "Epoch 84/500\n",
      "16908/16908 [==============================] - 296s 18ms/step - loss: 0.6739 - accuracy: 0.6954 - val_loss: 0.7733 - val_accuracy: 0.6317\n",
      "Epoch 85/500\n",
      "16908/16908 [==============================] - 294s 17ms/step - loss: 0.6724 - accuracy: 0.6945 - val_loss: 0.8112 - val_accuracy: 0.6210\n",
      "Epoch 86/500\n",
      "16908/16908 [==============================] - 295s 17ms/step - loss: 0.6749 - accuracy: 0.6937 - val_loss: 0.7617 - val_accuracy: 0.6418\n",
      "Epoch 87/500\n",
      "16908/16908 [==============================] - 289s 17ms/step - loss: 0.6660 - accuracy: 0.7010 - val_loss: 0.7542 - val_accuracy: 0.6532\n",
      "Epoch 88/500\n",
      "16908/16908 [==============================] - 285s 17ms/step - loss: 0.6642 - accuracy: 0.6985 - val_loss: 0.7517 - val_accuracy: 0.6541\n",
      "Epoch 89/500\n",
      "16908/16908 [==============================] - 290s 17ms/step - loss: 0.6603 - accuracy: 0.7043 - val_loss: 0.7640 - val_accuracy: 0.6449\n",
      "Epoch 90/500\n",
      "16908/16908 [==============================] - 290s 17ms/step - loss: 0.6619 - accuracy: 0.6980 - val_loss: 0.7420 - val_accuracy: 0.6662\n",
      "Epoch 91/500\n",
      "16908/16908 [==============================] - 290s 17ms/step - loss: 0.6607 - accuracy: 0.7010 - val_loss: 0.7525 - val_accuracy: 0.6560\n",
      "Epoch 92/500\n",
      "16908/16908 [==============================] - 301s 18ms/step - loss: 0.6559 - accuracy: 0.7065 - val_loss: 0.7400 - val_accuracy: 0.6726\n",
      "Epoch 93/500\n",
      "16908/16908 [==============================] - 311s 18ms/step - loss: 0.6490 - accuracy: 0.7058 - val_loss: 0.7314 - val_accuracy: 0.6714\n",
      "Epoch 94/500\n",
      "16908/16908 [==============================] - 295s 17ms/step - loss: 0.6510 - accuracy: 0.7069 - val_loss: 0.7313 - val_accuracy: 0.6752\n",
      "Epoch 95/500\n",
      "16908/16908 [==============================] - 295s 17ms/step - loss: 0.6451 - accuracy: 0.7110 - val_loss: 0.7262 - val_accuracy: 0.6830\n",
      "Epoch 96/500\n",
      "16908/16908 [==============================] - 296s 18ms/step - loss: 0.6460 - accuracy: 0.7087 - val_loss: 0.7182 - val_accuracy: 0.6832\n",
      "Epoch 97/500\n",
      "16908/16908 [==============================] - 288s 17ms/step - loss: 0.6465 - accuracy: 0.7096 - val_loss: 0.7578 - val_accuracy: 0.6487\n",
      "Epoch 98/500\n",
      "16908/16908 [==============================] - 286s 17ms/step - loss: 0.6388 - accuracy: 0.7122 - val_loss: 0.7564 - val_accuracy: 0.6537\n",
      "Epoch 99/500\n",
      "16908/16908 [==============================] - 286s 17ms/step - loss: 0.6381 - accuracy: 0.7116 - val_loss: 0.7433 - val_accuracy: 0.6686\n",
      "Epoch 100/500\n",
      "16908/16908 [==============================] - 282s 17ms/step - loss: 0.6343 - accuracy: 0.7198 - val_loss: 0.7099 - val_accuracy: 0.6913\n",
      "Epoch 101/500\n",
      "16908/16908 [==============================] - 291s 17ms/step - loss: 0.6315 - accuracy: 0.7174 - val_loss: 0.7167 - val_accuracy: 0.6816\n",
      "Epoch 102/500\n",
      "16908/16908 [==============================] - 291s 17ms/step - loss: 0.6281 - accuracy: 0.7187 - val_loss: 0.8044 - val_accuracy: 0.6061\n",
      "Epoch 103/500\n",
      "16908/16908 [==============================] - 283s 17ms/step - loss: 0.6254 - accuracy: 0.7216 - val_loss: 0.7149 - val_accuracy: 0.6887\n",
      "Epoch 104/500\n",
      "16908/16908 [==============================] - 288s 17ms/step - loss: 0.6260 - accuracy: 0.7196 - val_loss: 0.7126 - val_accuracy: 0.6891\n",
      "Epoch 105/500\n",
      "16908/16908 [==============================] - 284s 17ms/step - loss: 0.6244 - accuracy: 0.7208 - val_loss: 0.7170 - val_accuracy: 0.6752\n",
      "Epoch 106/500\n",
      "16908/16908 [==============================] - 285s 17ms/step - loss: 0.6207 - accuracy: 0.7243 - val_loss: 0.7303 - val_accuracy: 0.6721\n",
      "Epoch 107/500\n",
      "16908/16908 [==============================] - 283s 17ms/step - loss: 0.6222 - accuracy: 0.7205 - val_loss: 0.6972 - val_accuracy: 0.6991\n",
      "Epoch 108/500\n",
      "16908/16908 [==============================] - 282s 17ms/step - loss: 0.6177 - accuracy: 0.7263 - val_loss: 0.7456 - val_accuracy: 0.6570\n",
      "Epoch 109/500\n",
      "16908/16908 [==============================] - 282s 17ms/step - loss: 0.6130 - accuracy: 0.7264 - val_loss: 0.7143 - val_accuracy: 0.6877\n",
      "Epoch 110/500\n",
      "16908/16908 [==============================] - 283s 17ms/step - loss: 0.6098 - accuracy: 0.7285 - val_loss: 0.7007 - val_accuracy: 0.6917\n",
      "Epoch 111/500\n",
      "16908/16908 [==============================] - 280s 17ms/step - loss: 0.6124 - accuracy: 0.7277 - val_loss: 0.8053 - val_accuracy: 0.6167\n",
      "Epoch 112/500\n",
      "16908/16908 [==============================] - 278s 16ms/step - loss: 0.6065 - accuracy: 0.7341 - val_loss: 0.7258 - val_accuracy: 0.6688\n",
      "Epoch 113/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.6036 - accuracy: 0.7318 - val_loss: 0.7055 - val_accuracy: 0.6920\n",
      "Epoch 114/500\n",
      "16908/16908 [==============================] - 278s 16ms/step - loss: 0.6031 - accuracy: 0.7347 - val_loss: 0.7362 - val_accuracy: 0.6761\n",
      "Epoch 115/500\n",
      "16908/16908 [==============================] - 280s 17ms/step - loss: 0.6069 - accuracy: 0.7322 - val_loss: 0.6966 - val_accuracy: 0.6877\n",
      "Epoch 116/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.5982 - accuracy: 0.7352 - val_loss: 0.6982 - val_accuracy: 0.6915\n",
      "Epoch 117/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.5954 - accuracy: 0.7394 - val_loss: 0.7191 - val_accuracy: 0.6738\n",
      "Epoch 118/500\n",
      "16908/16908 [==============================] - 279s 17ms/step - loss: 0.5944 - accuracy: 0.7389 - val_loss: 0.7047 - val_accuracy: 0.6816\n",
      "Epoch 119/500\n",
      "16908/16908 [==============================] - 278s 16ms/step - loss: 0.5911 - accuracy: 0.7379 - val_loss: 0.6956 - val_accuracy: 0.6977\n",
      "Epoch 120/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.5874 - accuracy: 0.7391 - val_loss: 0.7048 - val_accuracy: 0.6749\n",
      "Epoch 121/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5858 - accuracy: 0.7425 - val_loss: 0.6942 - val_accuracy: 0.6884\n",
      "Epoch 122/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.5813 - accuracy: 0.7465 - val_loss: 0.6811 - val_accuracy: 0.7050\n",
      "Epoch 123/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.5806 - accuracy: 0.7462 - val_loss: 0.6899 - val_accuracy: 0.6955\n",
      "Epoch 124/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.5853 - accuracy: 0.7445 - val_loss: 0.6797 - val_accuracy: 0.7052\n",
      "Epoch 125/500\n",
      "16908/16908 [==============================] - 271s 16ms/step - loss: 0.5791 - accuracy: 0.7479 - val_loss: 0.6768 - val_accuracy: 0.7036\n",
      "Epoch 126/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.5714 - accuracy: 0.7496 - val_loss: 0.7124 - val_accuracy: 0.6780\n",
      "Epoch 127/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.5726 - accuracy: 0.7507 - val_loss: 0.6841 - val_accuracy: 0.7029\n",
      "Epoch 128/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5735 - accuracy: 0.7485 - val_loss: 0.6694 - val_accuracy: 0.7128\n",
      "Epoch 129/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5641 - accuracy: 0.7573 - val_loss: 0.6670 - val_accuracy: 0.7147\n",
      "Epoch 130/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5651 - accuracy: 0.7529 - val_loss: 0.7010 - val_accuracy: 0.6901\n",
      "Epoch 131/500\n",
      "16908/16908 [==============================] - 270s 16ms/step - loss: 0.5613 - accuracy: 0.7559 - val_loss: 0.6712 - val_accuracy: 0.7107\n",
      "Epoch 132/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5635 - accuracy: 0.7530 - val_loss: 0.6693 - val_accuracy: 0.7140\n",
      "Epoch 133/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.5594 - accuracy: 0.7566 - val_loss: 0.6607 - val_accuracy: 0.7156\n",
      "Epoch 134/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5532 - accuracy: 0.7595 - val_loss: 0.6710 - val_accuracy: 0.7100\n",
      "Epoch 135/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5474 - accuracy: 0.7600 - val_loss: 0.6576 - val_accuracy: 0.7225\n",
      "Epoch 136/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5555 - accuracy: 0.7601 - val_loss: 0.6671 - val_accuracy: 0.7071\n",
      "Epoch 137/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5488 - accuracy: 0.7654 - val_loss: 0.6587 - val_accuracy: 0.7152\n",
      "Epoch 138/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5462 - accuracy: 0.7637 - val_loss: 0.6657 - val_accuracy: 0.7064\n",
      "Epoch 139/500\n",
      "16908/16908 [==============================] - 270s 16ms/step - loss: 0.5466 - accuracy: 0.7635 - val_loss: 0.6578 - val_accuracy: 0.7126\n",
      "Epoch 140/500\n",
      "16908/16908 [==============================] - 270s 16ms/step - loss: 0.5376 - accuracy: 0.7694 - val_loss: 0.7021 - val_accuracy: 0.6716\n",
      "Epoch 141/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5376 - accuracy: 0.7702 - val_loss: 0.6343 - val_accuracy: 0.7310\n",
      "Epoch 142/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.5339 - accuracy: 0.7693 - val_loss: 0.6826 - val_accuracy: 0.6977\n",
      "Epoch 143/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5357 - accuracy: 0.7685 - val_loss: 0.6613 - val_accuracy: 0.7010\n",
      "Epoch 144/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5325 - accuracy: 0.7716 - val_loss: 0.6581 - val_accuracy: 0.7154\n",
      "Epoch 145/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5321 - accuracy: 0.7738 - val_loss: 0.6765 - val_accuracy: 0.7007\n",
      "Epoch 146/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.5250 - accuracy: 0.7740 - val_loss: 0.6787 - val_accuracy: 0.6934\n",
      "Epoch 147/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.5291 - accuracy: 0.7754 - val_loss: 0.6797 - val_accuracy: 0.6948\n",
      "Epoch 148/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.5167 - accuracy: 0.7792 - val_loss: 0.6304 - val_accuracy: 0.7386\n",
      "Epoch 149/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5252 - accuracy: 0.7770 - val_loss: 0.6485 - val_accuracy: 0.7234\n",
      "Epoch 150/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.5134 - accuracy: 0.7820 - val_loss: 0.6443 - val_accuracy: 0.7194\n",
      "Epoch 151/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.5141 - accuracy: 0.7792 - val_loss: 0.6745 - val_accuracy: 0.6946\n",
      "Epoch 152/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5199 - accuracy: 0.7783 - val_loss: 0.6318 - val_accuracy: 0.7249\n",
      "Epoch 153/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.5055 - accuracy: 0.7883 - val_loss: 0.6325 - val_accuracy: 0.7237\n",
      "Epoch 154/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.5038 - accuracy: 0.7850 - val_loss: 0.6205 - val_accuracy: 0.7308\n",
      "Epoch 155/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.5039 - accuracy: 0.7846 - val_loss: 0.6581 - val_accuracy: 0.6991\n",
      "Epoch 156/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.5039 - accuracy: 0.7860 - val_loss: 0.6461 - val_accuracy: 0.7225\n",
      "Epoch 157/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4996 - accuracy: 0.7888 - val_loss: 0.6161 - val_accuracy: 0.7419\n",
      "Epoch 158/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.4965 - accuracy: 0.7917 - val_loss: 0.6470 - val_accuracy: 0.7040\n",
      "Epoch 159/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4971 - accuracy: 0.7900 - val_loss: 0.6478 - val_accuracy: 0.7173\n",
      "Epoch 160/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4973 - accuracy: 0.7892 - val_loss: 0.6384 - val_accuracy: 0.7301\n",
      "Epoch 161/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.4910 - accuracy: 0.7909 - val_loss: 0.6149 - val_accuracy: 0.7402\n",
      "Epoch 162/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.4887 - accuracy: 0.7961 - val_loss: 0.6358 - val_accuracy: 0.7145\n",
      "Epoch 163/500\n",
      "16908/16908 [==============================] - 271s 16ms/step - loss: 0.4846 - accuracy: 0.7938 - val_loss: 0.6088 - val_accuracy: 0.7469\n",
      "Epoch 164/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4875 - accuracy: 0.7953 - val_loss: 0.6187 - val_accuracy: 0.7383\n",
      "Epoch 165/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4876 - accuracy: 0.7925 - val_loss: 0.6237 - val_accuracy: 0.7315\n",
      "Epoch 166/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.4793 - accuracy: 0.7996 - val_loss: 0.6271 - val_accuracy: 0.7315\n",
      "Epoch 167/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4768 - accuracy: 0.8011 - val_loss: 0.5955 - val_accuracy: 0.7563\n",
      "Epoch 168/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4718 - accuracy: 0.8016 - val_loss: 0.6304 - val_accuracy: 0.7324\n",
      "Epoch 169/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4745 - accuracy: 0.8033 - val_loss: 0.5942 - val_accuracy: 0.7445\n",
      "Epoch 170/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4608 - accuracy: 0.8081 - val_loss: 0.6240 - val_accuracy: 0.7310\n",
      "Epoch 171/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.4690 - accuracy: 0.8033 - val_loss: 0.6143 - val_accuracy: 0.7284\n",
      "Epoch 172/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.4683 - accuracy: 0.8031 - val_loss: 0.5893 - val_accuracy: 0.7608\n",
      "Epoch 173/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.4634 - accuracy: 0.8081 - val_loss: 0.5930 - val_accuracy: 0.7580\n",
      "Epoch 174/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.4576 - accuracy: 0.8094 - val_loss: 0.6062 - val_accuracy: 0.7440\n",
      "Epoch 175/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4541 - accuracy: 0.8122 - val_loss: 0.6111 - val_accuracy: 0.7424\n",
      "Epoch 176/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.4530 - accuracy: 0.8094 - val_loss: 0.6167 - val_accuracy: 0.7362\n",
      "Epoch 177/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.4495 - accuracy: 0.8148 - val_loss: 0.5915 - val_accuracy: 0.7573\n",
      "Epoch 178/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4532 - accuracy: 0.8106 - val_loss: 0.5864 - val_accuracy: 0.7535\n",
      "Epoch 179/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4522 - accuracy: 0.8103 - val_loss: 0.5742 - val_accuracy: 0.7644\n",
      "Epoch 180/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4495 - accuracy: 0.8120 - val_loss: 0.5826 - val_accuracy: 0.7573\n",
      "Epoch 181/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.4504 - accuracy: 0.8114 - val_loss: 0.5865 - val_accuracy: 0.7561\n",
      "Epoch 182/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.4369 - accuracy: 0.8185 - val_loss: 0.5958 - val_accuracy: 0.7514\n",
      "Epoch 183/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.4315 - accuracy: 0.8192 - val_loss: 0.5818 - val_accuracy: 0.7611\n",
      "Epoch 184/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4352 - accuracy: 0.8230 - val_loss: 0.5706 - val_accuracy: 0.7639\n",
      "Epoch 185/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.4347 - accuracy: 0.8206 - val_loss: 0.5728 - val_accuracy: 0.7613\n",
      "Epoch 186/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4343 - accuracy: 0.8220 - val_loss: 0.5830 - val_accuracy: 0.7622\n",
      "Epoch 187/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.4305 - accuracy: 0.8219 - val_loss: 0.5750 - val_accuracy: 0.7620\n",
      "Epoch 188/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4240 - accuracy: 0.8246 - val_loss: 0.5870 - val_accuracy: 0.7547\n",
      "Epoch 189/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.4251 - accuracy: 0.8236 - val_loss: 0.5782 - val_accuracy: 0.7585\n",
      "Epoch 190/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.4264 - accuracy: 0.8254 - val_loss: 0.5972 - val_accuracy: 0.7495\n",
      "Epoch 191/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.4160 - accuracy: 0.8280 - val_loss: 0.5883 - val_accuracy: 0.7492\n",
      "Epoch 192/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4203 - accuracy: 0.8258 - val_loss: 0.5905 - val_accuracy: 0.7533\n",
      "Epoch 193/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4095 - accuracy: 0.8329 - val_loss: 0.5650 - val_accuracy: 0.7753\n",
      "Epoch 194/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.4119 - accuracy: 0.8332 - val_loss: 0.5666 - val_accuracy: 0.7644\n",
      "Epoch 195/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.4103 - accuracy: 0.8329 - val_loss: 0.5797 - val_accuracy: 0.7535\n",
      "Epoch 196/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.4054 - accuracy: 0.8368 - val_loss: 0.5927 - val_accuracy: 0.7485\n",
      "Epoch 197/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.4024 - accuracy: 0.8389 - val_loss: 0.5531 - val_accuracy: 0.7630\n",
      "Epoch 198/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.4011 - accuracy: 0.8351 - val_loss: 0.5743 - val_accuracy: 0.7528\n",
      "Epoch 199/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.3977 - accuracy: 0.8399 - val_loss: 0.5563 - val_accuracy: 0.7727\n",
      "Epoch 200/500\n",
      "16908/16908 [==============================] - 281s 17ms/step - loss: 0.3972 - accuracy: 0.8408 - val_loss: 0.5563 - val_accuracy: 0.7596\n",
      "Epoch 201/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3907 - accuracy: 0.8424 - val_loss: 0.6178 - val_accuracy: 0.7166\n",
      "Epoch 202/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3933 - accuracy: 0.8414 - val_loss: 0.5564 - val_accuracy: 0.7698\n",
      "Epoch 203/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3884 - accuracy: 0.8423 - val_loss: 0.5459 - val_accuracy: 0.7776\n",
      "Epoch 204/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.3916 - accuracy: 0.8391 - val_loss: 0.5410 - val_accuracy: 0.7892\n",
      "Epoch 205/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.3888 - accuracy: 0.8428 - val_loss: 0.6117 - val_accuracy: 0.7357\n",
      "Epoch 206/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.3846 - accuracy: 0.8463 - val_loss: 0.5676 - val_accuracy: 0.7601\n",
      "Epoch 207/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.3809 - accuracy: 0.8463 - val_loss: 0.5301 - val_accuracy: 0.7868\n",
      "Epoch 208/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3832 - accuracy: 0.8463 - val_loss: 0.5270 - val_accuracy: 0.7954\n",
      "Epoch 209/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.3814 - accuracy: 0.8432 - val_loss: 0.5341 - val_accuracy: 0.7864\n",
      "Epoch 210/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3733 - accuracy: 0.8514 - val_loss: 0.5407 - val_accuracy: 0.7868\n",
      "Epoch 211/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3740 - accuracy: 0.8515 - val_loss: 0.5293 - val_accuracy: 0.7824\n",
      "Epoch 212/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3706 - accuracy: 0.8526 - val_loss: 0.5351 - val_accuracy: 0.7807\n",
      "Epoch 213/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.3651 - accuracy: 0.8536 - val_loss: 0.5240 - val_accuracy: 0.7868\n",
      "Epoch 214/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.3653 - accuracy: 0.8529 - val_loss: 0.5550 - val_accuracy: 0.7563\n",
      "Epoch 215/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.3602 - accuracy: 0.8536 - val_loss: 0.5286 - val_accuracy: 0.7911\n",
      "Epoch 216/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3581 - accuracy: 0.8568 - val_loss: 0.5667 - val_accuracy: 0.7634\n",
      "Epoch 217/500\n",
      "16908/16908 [==============================] - 271s 16ms/step - loss: 0.3559 - accuracy: 0.8579 - val_loss: 0.5422 - val_accuracy: 0.7729\n",
      "Epoch 218/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3570 - accuracy: 0.8588 - val_loss: 0.5115 - val_accuracy: 0.7935\n",
      "Epoch 219/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.3498 - accuracy: 0.8608 - val_loss: 0.5204 - val_accuracy: 0.7894\n",
      "Epoch 220/500\n",
      "16908/16908 [==============================] - 271s 16ms/step - loss: 0.3531 - accuracy: 0.8574 - val_loss: 0.5321 - val_accuracy: 0.7871\n",
      "Epoch 221/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.3463 - accuracy: 0.8633 - val_loss: 0.5110 - val_accuracy: 0.7963\n",
      "Epoch 222/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3402 - accuracy: 0.8657 - val_loss: 0.5209 - val_accuracy: 0.7854\n",
      "Epoch 223/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.3428 - accuracy: 0.8663 - val_loss: 0.4980 - val_accuracy: 0.8013\n",
      "Epoch 224/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3419 - accuracy: 0.8644 - val_loss: 0.5105 - val_accuracy: 0.7958\n",
      "Epoch 225/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3410 - accuracy: 0.8640 - val_loss: 0.5103 - val_accuracy: 0.7999\n",
      "Epoch 226/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.3383 - accuracy: 0.8666 - val_loss: 0.5787 - val_accuracy: 0.7428\n",
      "Epoch 227/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3333 - accuracy: 0.8661 - val_loss: 0.5080 - val_accuracy: 0.7937\n",
      "Epoch 228/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3356 - accuracy: 0.8682 - val_loss: 0.5338 - val_accuracy: 0.7776\n",
      "Epoch 229/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.3276 - accuracy: 0.8692 - val_loss: 0.5180 - val_accuracy: 0.7812\n",
      "Epoch 230/500\n",
      "16908/16908 [==============================] - 269s 16ms/step - loss: 0.3278 - accuracy: 0.8733 - val_loss: 0.5042 - val_accuracy: 0.7999\n",
      "Epoch 231/500\n",
      "16908/16908 [==============================] - 270s 16ms/step - loss: 0.3219 - accuracy: 0.8711 - val_loss: 0.5020 - val_accuracy: 0.8015\n",
      "Epoch 232/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3285 - accuracy: 0.8689 - val_loss: 0.5532 - val_accuracy: 0.7774\n",
      "Epoch 233/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.3207 - accuracy: 0.8747 - val_loss: 0.4833 - val_accuracy: 0.8145\n",
      "Epoch 234/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3276 - accuracy: 0.8726 - val_loss: 0.4930 - val_accuracy: 0.8039\n",
      "Epoch 235/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3239 - accuracy: 0.8719 - val_loss: 0.4899 - val_accuracy: 0.8171\n",
      "Epoch 236/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.3144 - accuracy: 0.8754 - val_loss: 0.4901 - val_accuracy: 0.8022\n",
      "Epoch 237/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.3199 - accuracy: 0.8711 - val_loss: 0.4919 - val_accuracy: 0.8115\n",
      "Epoch 238/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3124 - accuracy: 0.8770 - val_loss: 0.4879 - val_accuracy: 0.8129\n",
      "Epoch 239/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3111 - accuracy: 0.8787 - val_loss: 0.5081 - val_accuracy: 0.7911\n",
      "Epoch 240/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3062 - accuracy: 0.8799 - val_loss: 0.4952 - val_accuracy: 0.8058\n",
      "Epoch 241/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.3093 - accuracy: 0.8771 - val_loss: 0.5083 - val_accuracy: 0.7994\n",
      "Epoch 242/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.3059 - accuracy: 0.8806 - val_loss: 0.4786 - val_accuracy: 0.8105\n",
      "Epoch 243/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.3007 - accuracy: 0.8841 - val_loss: 0.4683 - val_accuracy: 0.8133\n",
      "Epoch 244/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2922 - accuracy: 0.8874 - val_loss: 0.4725 - val_accuracy: 0.8174\n",
      "Epoch 245/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.2971 - accuracy: 0.8857 - val_loss: 0.4966 - val_accuracy: 0.7975\n",
      "Epoch 246/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2957 - accuracy: 0.8852 - val_loss: 0.5188 - val_accuracy: 0.7975\n",
      "Epoch 247/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.2925 - accuracy: 0.8860 - val_loss: 0.4967 - val_accuracy: 0.8032\n",
      "Epoch 248/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.2876 - accuracy: 0.8888 - val_loss: 0.5141 - val_accuracy: 0.7807\n",
      "Epoch 249/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.2878 - accuracy: 0.8899 - val_loss: 0.4670 - val_accuracy: 0.8148\n",
      "Epoch 250/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.2859 - accuracy: 0.8874 - val_loss: 0.5229 - val_accuracy: 0.7866\n",
      "Epoch 251/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.2863 - accuracy: 0.8873 - val_loss: 0.4901 - val_accuracy: 0.8126\n",
      "Epoch 252/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.2835 - accuracy: 0.8902 - val_loss: 0.4619 - val_accuracy: 0.8249\n",
      "Epoch 253/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.2865 - accuracy: 0.8895 - val_loss: 0.4899 - val_accuracy: 0.8112\n",
      "Epoch 254/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.2764 - accuracy: 0.8944 - val_loss: 0.4664 - val_accuracy: 0.8117\n",
      "Epoch 255/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.2826 - accuracy: 0.8912 - val_loss: 0.4726 - val_accuracy: 0.8145\n",
      "Epoch 256/500\n",
      "16908/16908 [==============================] - 270s 16ms/step - loss: 0.2730 - accuracy: 0.8951 - val_loss: 0.4972 - val_accuracy: 0.8022\n",
      "Epoch 257/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.2772 - accuracy: 0.8960 - val_loss: 0.4592 - val_accuracy: 0.8245\n",
      "Epoch 258/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2707 - accuracy: 0.8951 - val_loss: 0.4514 - val_accuracy: 0.8332\n",
      "Epoch 259/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.2716 - accuracy: 0.8946 - val_loss: 0.4688 - val_accuracy: 0.8100\n",
      "Epoch 260/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2628 - accuracy: 0.8987 - val_loss: 0.4665 - val_accuracy: 0.8207\n",
      "Epoch 261/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.2695 - accuracy: 0.8962 - val_loss: 0.4518 - val_accuracy: 0.8242\n",
      "Epoch 262/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2638 - accuracy: 0.8971 - val_loss: 0.4547 - val_accuracy: 0.8204\n",
      "Epoch 263/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.2664 - accuracy: 0.8954 - val_loss: 0.4642 - val_accuracy: 0.8162\n",
      "Epoch 264/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2572 - accuracy: 0.9022 - val_loss: 0.4672 - val_accuracy: 0.8143\n",
      "Epoch 265/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.2597 - accuracy: 0.9003 - val_loss: 0.4703 - val_accuracy: 0.8176\n",
      "Epoch 266/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.2557 - accuracy: 0.9028 - val_loss: 0.4843 - val_accuracy: 0.8084\n",
      "Epoch 267/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2561 - accuracy: 0.9012 - val_loss: 0.4519 - val_accuracy: 0.8280\n",
      "Epoch 268/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.2515 - accuracy: 0.9008 - val_loss: 0.5059 - val_accuracy: 0.8022\n",
      "Epoch 269/500\n",
      "16908/16908 [==============================] - 377s 22ms/step - loss: 0.2491 - accuracy: 0.9038 - val_loss: 0.4221 - val_accuracy: 0.8429\n",
      "Epoch 270/500\n",
      "16908/16908 [==============================] - 376s 22ms/step - loss: 0.2468 - accuracy: 0.9059 - val_loss: 0.4533 - val_accuracy: 0.8197\n",
      "Epoch 271/500\n",
      "16908/16908 [==============================] - 341s 20ms/step - loss: 0.2482 - accuracy: 0.9062 - val_loss: 0.4375 - val_accuracy: 0.8391\n",
      "Epoch 272/500\n",
      "16908/16908 [==============================] - 331s 20ms/step - loss: 0.2547 - accuracy: 0.9032 - val_loss: 0.4393 - val_accuracy: 0.8290\n",
      "Epoch 273/500\n",
      "16908/16908 [==============================] - 333s 20ms/step - loss: 0.2498 - accuracy: 0.9036 - val_loss: 0.4563 - val_accuracy: 0.8252\n",
      "Epoch 274/500\n",
      "16908/16908 [==============================] - 415s 25ms/step - loss: 0.2369 - accuracy: 0.9092 - val_loss: 0.4670 - val_accuracy: 0.8072\n",
      "Epoch 275/500\n",
      "16908/16908 [==============================] - 374s 22ms/step - loss: 0.2478 - accuracy: 0.9049 - val_loss: 0.4464 - val_accuracy: 0.8297\n",
      "Epoch 276/500\n",
      "16908/16908 [==============================] - 344s 20ms/step - loss: 0.2421 - accuracy: 0.9072 - val_loss: 0.4452 - val_accuracy: 0.8271\n",
      "Epoch 277/500\n",
      "16908/16908 [==============================] - 279s 17ms/step - loss: 0.2419 - accuracy: 0.9074 - val_loss: 0.4297 - val_accuracy: 0.8387\n",
      "Epoch 278/500\n",
      "16908/16908 [==============================] - 402s 24ms/step - loss: 0.2358 - accuracy: 0.9119 - val_loss: 0.4384 - val_accuracy: 0.8290\n",
      "Epoch 279/500\n",
      "16908/16908 [==============================] - 462s 27ms/step - loss: 0.2339 - accuracy: 0.9116 - val_loss: 0.4395 - val_accuracy: 0.8287\n",
      "Epoch 280/500\n",
      "16908/16908 [==============================] - 320s 19ms/step - loss: 0.2318 - accuracy: 0.9134 - val_loss: 0.4179 - val_accuracy: 0.8453\n",
      "Epoch 281/500\n",
      "16908/16908 [==============================] - 282s 17ms/step - loss: 0.2325 - accuracy: 0.9097 - val_loss: 0.4281 - val_accuracy: 0.8396\n",
      "Epoch 282/500\n",
      "16908/16908 [==============================] - 285s 17ms/step - loss: 0.2226 - accuracy: 0.9186 - val_loss: 0.4348 - val_accuracy: 0.8368\n",
      "Epoch 283/500\n",
      "16908/16908 [==============================] - 288s 17ms/step - loss: 0.2323 - accuracy: 0.9097 - val_loss: 0.4324 - val_accuracy: 0.8410\n",
      "Epoch 284/500\n",
      "16908/16908 [==============================] - 298s 18ms/step - loss: 0.2268 - accuracy: 0.9147 - val_loss: 0.4423 - val_accuracy: 0.8290\n",
      "Epoch 285/500\n",
      "16908/16908 [==============================] - 281s 17ms/step - loss: 0.2224 - accuracy: 0.9160 - val_loss: 0.4307 - val_accuracy: 0.8346\n",
      "Epoch 286/500\n",
      "16908/16908 [==============================] - 283s 17ms/step - loss: 0.2237 - accuracy: 0.9155 - val_loss: 0.4258 - val_accuracy: 0.8391\n",
      "Epoch 287/500\n",
      "16908/16908 [==============================] - 334s 20ms/step - loss: 0.2193 - accuracy: 0.9178 - val_loss: 0.4303 - val_accuracy: 0.8408\n",
      "Epoch 288/500\n",
      "16908/16908 [==============================] - 323s 19ms/step - loss: 0.2159 - accuracy: 0.9163 - val_loss: 0.4350 - val_accuracy: 0.8370\n",
      "Epoch 289/500\n",
      "16908/16908 [==============================] - 257s 15ms/step - loss: 0.2171 - accuracy: 0.9178 - val_loss: 0.4375 - val_accuracy: 0.8308\n",
      "Epoch 290/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2146 - accuracy: 0.9182 - val_loss: 0.4229 - val_accuracy: 0.8413\n",
      "Epoch 291/500\n",
      "16908/16908 [==============================] - 282s 17ms/step - loss: 0.2194 - accuracy: 0.9178 - val_loss: 0.4285 - val_accuracy: 0.8358\n",
      "Epoch 292/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.2159 - accuracy: 0.9194 - val_loss: 0.4428 - val_accuracy: 0.8313\n",
      "Epoch 293/500\n",
      "16908/16908 [==============================] - 254s 15ms/step - loss: 0.2156 - accuracy: 0.9173 - val_loss: 0.4085 - val_accuracy: 0.8500\n",
      "Epoch 294/500\n",
      "16908/16908 [==============================] - 279s 16ms/step - loss: 0.2095 - accuracy: 0.9224 - val_loss: 0.4044 - val_accuracy: 0.8562\n",
      "Epoch 295/500\n",
      "16908/16908 [==============================] - 338s 20ms/step - loss: 0.2076 - accuracy: 0.9216 - val_loss: 0.4077 - val_accuracy: 0.8484\n",
      "Epoch 296/500\n",
      "16908/16908 [==============================] - 252s 15ms/step - loss: 0.2040 - accuracy: 0.9244 - val_loss: 0.4265 - val_accuracy: 0.8358\n",
      "Epoch 297/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.2053 - accuracy: 0.9215 - val_loss: 0.4089 - val_accuracy: 0.8432\n",
      "Epoch 298/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.2011 - accuracy: 0.9251 - val_loss: 0.4737 - val_accuracy: 0.8138\n",
      "Epoch 299/500\n",
      "16908/16908 [==============================] - 275s 16ms/step - loss: 0.2010 - accuracy: 0.9232 - val_loss: 0.4458 - val_accuracy: 0.8268\n",
      "Epoch 300/500\n",
      "16908/16908 [==============================] - 278s 16ms/step - loss: 0.2015 - accuracy: 0.9234 - val_loss: 0.4044 - val_accuracy: 0.8547\n",
      "Epoch 301/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.2036 - accuracy: 0.9229 - val_loss: 0.4590 - val_accuracy: 0.8145\n",
      "Epoch 302/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.1946 - accuracy: 0.9284 - val_loss: 0.4205 - val_accuracy: 0.8439\n",
      "Epoch 303/500\n",
      "16908/16908 [==============================] - 296s 17ms/step - loss: 0.2006 - accuracy: 0.9252 - val_loss: 0.3927 - val_accuracy: 0.8552\n",
      "Epoch 304/500\n",
      "16908/16908 [==============================] - 269s 16ms/step - loss: 0.1906 - accuracy: 0.9292 - val_loss: 0.3955 - val_accuracy: 0.8581\n",
      "Epoch 305/500\n",
      "16908/16908 [==============================] - 254s 15ms/step - loss: 0.1921 - accuracy: 0.9300 - val_loss: 0.3870 - val_accuracy: 0.8621\n",
      "Epoch 306/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.1933 - accuracy: 0.9261 - val_loss: 0.4206 - val_accuracy: 0.8436\n",
      "Epoch 307/500\n",
      "16908/16908 [==============================] - 254s 15ms/step - loss: 0.1963 - accuracy: 0.9282 - val_loss: 0.4405 - val_accuracy: 0.8358\n",
      "Epoch 308/500\n",
      "16908/16908 [==============================] - 257s 15ms/step - loss: 0.1918 - accuracy: 0.9296 - val_loss: 0.4031 - val_accuracy: 0.8493\n",
      "Epoch 309/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.1837 - accuracy: 0.9326 - val_loss: 0.3917 - val_accuracy: 0.8533\n",
      "Epoch 310/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.1835 - accuracy: 0.9315 - val_loss: 0.3920 - val_accuracy: 0.8595\n",
      "Epoch 311/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.1824 - accuracy: 0.9331 - val_loss: 0.4116 - val_accuracy: 0.8476\n",
      "Epoch 312/500\n",
      "16908/16908 [==============================] - 287s 17ms/step - loss: 0.1798 - accuracy: 0.9339 - val_loss: 0.3834 - val_accuracy: 0.8583\n",
      "Epoch 313/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.1855 - accuracy: 0.9306 - val_loss: 0.3970 - val_accuracy: 0.8467\n",
      "Epoch 314/500\n",
      "16908/16908 [==============================] - 283s 17ms/step - loss: 0.1797 - accuracy: 0.9338 - val_loss: 0.3771 - val_accuracy: 0.8633\n",
      "Epoch 315/500\n",
      "16908/16908 [==============================] - 260s 15ms/step - loss: 0.1799 - accuracy: 0.9320 - val_loss: 0.4036 - val_accuracy: 0.8493\n",
      "Epoch 316/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.1802 - accuracy: 0.9315 - val_loss: 0.4213 - val_accuracy: 0.8469\n",
      "Epoch 317/500\n",
      "16908/16908 [==============================] - 284s 17ms/step - loss: 0.1807 - accuracy: 0.9335 - val_loss: 0.3900 - val_accuracy: 0.8581\n",
      "Epoch 318/500\n",
      "16908/16908 [==============================] - 265s 16ms/step - loss: 0.1788 - accuracy: 0.9344 - val_loss: 0.3732 - val_accuracy: 0.8675\n",
      "Epoch 319/500\n",
      "16908/16908 [==============================] - 284s 17ms/step - loss: 0.1721 - accuracy: 0.9363 - val_loss: 0.4061 - val_accuracy: 0.8462\n",
      "Epoch 320/500\n",
      "16908/16908 [==============================] - 250s 15ms/step - loss: 0.1721 - accuracy: 0.9359 - val_loss: 0.3760 - val_accuracy: 0.8614\n",
      "Epoch 321/500\n",
      "16908/16908 [==============================] - 247s 15ms/step - loss: 0.1737 - accuracy: 0.9353 - val_loss: 0.4071 - val_accuracy: 0.8453\n",
      "Epoch 322/500\n",
      "16908/16908 [==============================] - 266s 16ms/step - loss: 0.1681 - accuracy: 0.9396 - val_loss: 0.3825 - val_accuracy: 0.8590\n",
      "Epoch 323/500\n",
      "16908/16908 [==============================] - 265s 16ms/step - loss: 0.1723 - accuracy: 0.9376 - val_loss: 0.3901 - val_accuracy: 0.8540\n",
      "Epoch 324/500\n",
      "16908/16908 [==============================] - 265s 16ms/step - loss: 0.1640 - accuracy: 0.9397 - val_loss: 0.4334 - val_accuracy: 0.8313\n",
      "Epoch 325/500\n",
      "16908/16908 [==============================] - 264s 16ms/step - loss: 0.1688 - accuracy: 0.9369 - val_loss: 0.3750 - val_accuracy: 0.8604\n",
      "Epoch 326/500\n",
      "16908/16908 [==============================] - 266s 16ms/step - loss: 0.1600 - accuracy: 0.9399 - val_loss: 0.3867 - val_accuracy: 0.8588\n",
      "Epoch 327/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.1641 - accuracy: 0.9404 - val_loss: 0.4048 - val_accuracy: 0.8493\n",
      "Epoch 328/500\n",
      "16908/16908 [==============================] - 265s 16ms/step - loss: 0.1603 - accuracy: 0.9407 - val_loss: 0.3729 - val_accuracy: 0.8649\n",
      "Epoch 329/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.1640 - accuracy: 0.9410 - val_loss: 0.3661 - val_accuracy: 0.8685\n",
      "Epoch 330/500\n",
      "16908/16908 [==============================] - 288s 17ms/step - loss: 0.1673 - accuracy: 0.9380 - val_loss: 0.4093 - val_accuracy: 0.8491\n",
      "Epoch 331/500\n",
      "16908/16908 [==============================] - 288s 17ms/step - loss: 0.1556 - accuracy: 0.9432 - val_loss: 0.3658 - val_accuracy: 0.8621\n",
      "Epoch 332/500\n",
      "16908/16908 [==============================] - 290s 17ms/step - loss: 0.1632 - accuracy: 0.9402 - val_loss: 0.4159 - val_accuracy: 0.8450\n",
      "Epoch 333/500\n",
      "16908/16908 [==============================] - 278s 16ms/step - loss: 0.1572 - accuracy: 0.9429 - val_loss: 0.3843 - val_accuracy: 0.8547\n",
      "Epoch 334/500\n",
      "16908/16908 [==============================] - 263s 16ms/step - loss: 0.1533 - accuracy: 0.9445 - val_loss: 0.3783 - val_accuracy: 0.8616\n",
      "Epoch 335/500\n",
      "16908/16908 [==============================] - 277s 16ms/step - loss: 0.1596 - accuracy: 0.9417 - val_loss: 0.3860 - val_accuracy: 0.8592\n",
      "Epoch 336/500\n",
      "16908/16908 [==============================] - 283s 17ms/step - loss: 0.1521 - accuracy: 0.9425 - val_loss: 0.3687 - val_accuracy: 0.8661\n",
      "Epoch 337/500\n",
      "16908/16908 [==============================] - 246s 15ms/step - loss: 0.1491 - accuracy: 0.9451 - val_loss: 0.3746 - val_accuracy: 0.8637\n",
      "Epoch 338/500\n",
      "16908/16908 [==============================] - 261s 15ms/step - loss: 0.1564 - accuracy: 0.9430 - val_loss: 0.3644 - val_accuracy: 0.8678\n",
      "Epoch 339/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.1496 - accuracy: 0.9459 - val_loss: 0.3576 - val_accuracy: 0.8718\n",
      "Epoch 340/500\n",
      "16908/16908 [==============================] - 270s 16ms/step - loss: 0.1470 - accuracy: 0.9467 - val_loss: 0.4102 - val_accuracy: 0.8484\n",
      "Epoch 341/500\n",
      "16908/16908 [==============================] - 244s 14ms/step - loss: 0.1498 - accuracy: 0.9472 - val_loss: 0.3884 - val_accuracy: 0.8618\n",
      "Epoch 342/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.1461 - accuracy: 0.9456 - val_loss: 0.3691 - val_accuracy: 0.8685\n",
      "Epoch 343/500\n",
      "16908/16908 [==============================] - 283s 17ms/step - loss: 0.1480 - accuracy: 0.9454 - val_loss: 0.3774 - val_accuracy: 0.8661\n",
      "Epoch 344/500\n",
      "16908/16908 [==============================] - 286s 17ms/step - loss: 0.1480 - accuracy: 0.9455 - val_loss: 0.3705 - val_accuracy: 0.8666\n",
      "Epoch 345/500\n",
      "16908/16908 [==============================] - 270s 16ms/step - loss: 0.1458 - accuracy: 0.9472 - val_loss: 0.3801 - val_accuracy: 0.8602\n",
      "Epoch 346/500\n",
      "16908/16908 [==============================] - 291s 17ms/step - loss: 0.1400 - accuracy: 0.9504 - val_loss: 0.3860 - val_accuracy: 0.8604\n",
      "Epoch 347/500\n",
      "16908/16908 [==============================] - 279s 16ms/step - loss: 0.1519 - accuracy: 0.9457 - val_loss: 0.3522 - val_accuracy: 0.8732\n",
      "Epoch 348/500\n",
      "16908/16908 [==============================] - 255s 15ms/step - loss: 0.1396 - accuracy: 0.9490 - val_loss: 0.3773 - val_accuracy: 0.8630\n",
      "Epoch 349/500\n",
      "16908/16908 [==============================] - 253s 15ms/step - loss: 0.1366 - accuracy: 0.9505 - val_loss: 0.3522 - val_accuracy: 0.8727\n",
      "Epoch 350/500\n",
      "16908/16908 [==============================] - 288s 17ms/step - loss: 0.1429 - accuracy: 0.9498 - val_loss: 0.3518 - val_accuracy: 0.8713\n",
      "Epoch 351/500\n",
      "16908/16908 [==============================] - 302s 18ms/step - loss: 0.1397 - accuracy: 0.9484 - val_loss: 0.3571 - val_accuracy: 0.8694\n",
      "Epoch 352/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.1400 - accuracy: 0.9471 - val_loss: 0.4004 - val_accuracy: 0.8529\n",
      "Epoch 353/500\n",
      "16908/16908 [==============================] - 287s 17ms/step - loss: 0.1365 - accuracy: 0.9510 - val_loss: 0.3752 - val_accuracy: 0.8618\n",
      "Epoch 354/500\n",
      "16908/16908 [==============================] - 287s 17ms/step - loss: 0.1310 - accuracy: 0.9526 - val_loss: 0.3630 - val_accuracy: 0.8668\n",
      "Epoch 355/500\n",
      "16908/16908 [==============================] - 278s 16ms/step - loss: 0.1337 - accuracy: 0.9524 - val_loss: 0.3779 - val_accuracy: 0.8642\n",
      "Epoch 356/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.1388 - accuracy: 0.9483 - val_loss: 0.3907 - val_accuracy: 0.8529\n",
      "Epoch 357/500\n",
      "16908/16908 [==============================] - 253s 15ms/step - loss: 0.1351 - accuracy: 0.9509 - val_loss: 0.3746 - val_accuracy: 0.8614\n",
      "Epoch 358/500\n",
      "16908/16908 [==============================] - 372s 22ms/step - loss: 0.1382 - accuracy: 0.9502 - val_loss: 0.3575 - val_accuracy: 0.8680\n",
      "Epoch 359/500\n",
      "16908/16908 [==============================] - 407s 24ms/step - loss: 0.1400 - accuracy: 0.9508 - val_loss: 0.3599 - val_accuracy: 0.8718\n",
      "Epoch 360/500\n",
      "16908/16908 [==============================] - 307s 18ms/step - loss: 0.1325 - accuracy: 0.9523 - val_loss: 0.3692 - val_accuracy: 0.8659\n",
      "Epoch 361/500\n",
      "16908/16908 [==============================] - 271s 16ms/step - loss: 0.1287 - accuracy: 0.9539 - val_loss: 0.3507 - val_accuracy: 0.8708\n",
      "Epoch 362/500\n",
      "16908/16908 [==============================] - 313s 19ms/step - loss: 0.1290 - accuracy: 0.9529 - val_loss: 0.3579 - val_accuracy: 0.8689\n",
      "Epoch 363/500\n",
      "16908/16908 [==============================] - 260s 15ms/step - loss: 0.1341 - accuracy: 0.9513 - val_loss: 0.4060 - val_accuracy: 0.8557\n",
      "Epoch 364/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.1250 - accuracy: 0.9539 - val_loss: 0.3513 - val_accuracy: 0.8722\n",
      "Epoch 365/500\n",
      "16908/16908 [==============================] - 251s 15ms/step - loss: 0.1242 - accuracy: 0.9562 - val_loss: 0.3976 - val_accuracy: 0.8474\n",
      "Epoch 366/500\n",
      "16908/16908 [==============================] - 301s 18ms/step - loss: 0.1255 - accuracy: 0.9540 - val_loss: 0.3650 - val_accuracy: 0.8670\n",
      "Epoch 367/500\n",
      "16908/16908 [==============================] - 291s 17ms/step - loss: 0.1263 - accuracy: 0.9551 - val_loss: 0.3552 - val_accuracy: 0.8687\n",
      "Epoch 368/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.1273 - accuracy: 0.9546 - val_loss: 0.3811 - val_accuracy: 0.8609\n",
      "Epoch 369/500\n",
      "16908/16908 [==============================] - 287s 17ms/step - loss: 0.1264 - accuracy: 0.9547 - val_loss: 0.3437 - val_accuracy: 0.8737\n",
      "Epoch 370/500\n",
      "16908/16908 [==============================] - 272s 16ms/step - loss: 0.1200 - accuracy: 0.9577 - val_loss: 0.3866 - val_accuracy: 0.8583\n",
      "Epoch 371/500\n",
      "16908/16908 [==============================] - 263s 16ms/step - loss: 0.1198 - accuracy: 0.9553 - val_loss: 0.3431 - val_accuracy: 0.8744\n",
      "Epoch 372/500\n",
      "16908/16908 [==============================] - 247s 15ms/step - loss: 0.1244 - accuracy: 0.9569 - val_loss: 0.3491 - val_accuracy: 0.8765\n",
      "Epoch 373/500\n",
      "16908/16908 [==============================] - 269s 16ms/step - loss: 0.1232 - accuracy: 0.9553 - val_loss: 0.3818 - val_accuracy: 0.8607\n",
      "Epoch 374/500\n",
      "16908/16908 [==============================] - 290s 17ms/step - loss: 0.1168 - accuracy: 0.9574 - val_loss: 0.3551 - val_accuracy: 0.8744\n",
      "Epoch 375/500\n",
      "16908/16908 [==============================] - 264s 16ms/step - loss: 0.1174 - accuracy: 0.9579 - val_loss: 0.3807 - val_accuracy: 0.8611\n",
      "Epoch 376/500\n",
      "16908/16908 [==============================] - 262s 16ms/step - loss: 0.1213 - accuracy: 0.9565 - val_loss: 0.3743 - val_accuracy: 0.8621\n",
      "Epoch 377/500\n",
      "16908/16908 [==============================] - 257s 15ms/step - loss: 0.1155 - accuracy: 0.9588 - val_loss: 0.3644 - val_accuracy: 0.8661\n",
      "Epoch 378/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.1218 - accuracy: 0.9569 - val_loss: 0.3518 - val_accuracy: 0.8715\n",
      "Epoch 379/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.1127 - accuracy: 0.9589 - val_loss: 0.3636 - val_accuracy: 0.8689\n",
      "Epoch 380/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.1215 - accuracy: 0.9559 - val_loss: 0.3815 - val_accuracy: 0.8656\n",
      "Epoch 381/500\n",
      "16908/16908 [==============================] - 266s 16ms/step - loss: 0.1127 - accuracy: 0.9605 - val_loss: 0.3564 - val_accuracy: 0.8706\n",
      "Epoch 382/500\n",
      "16908/16908 [==============================] - 269s 16ms/step - loss: 0.1109 - accuracy: 0.9594 - val_loss: 0.3431 - val_accuracy: 0.8760\n",
      "Epoch 383/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.1118 - accuracy: 0.9617 - val_loss: 0.3818 - val_accuracy: 0.8609\n",
      "Epoch 384/500\n",
      "16908/16908 [==============================] - 256s 15ms/step - loss: 0.1128 - accuracy: 0.9599 - val_loss: 0.3433 - val_accuracy: 0.8739\n",
      "Epoch 385/500\n",
      "16908/16908 [==============================] - 266s 16ms/step - loss: 0.1186 - accuracy: 0.9570 - val_loss: 0.3585 - val_accuracy: 0.8706\n",
      "Epoch 386/500\n",
      "16908/16908 [==============================] - 296s 18ms/step - loss: 0.1166 - accuracy: 0.9573 - val_loss: 0.3361 - val_accuracy: 0.8749\n",
      "Epoch 387/500\n",
      "16908/16908 [==============================] - 251s 15ms/step - loss: 0.1133 - accuracy: 0.9584 - val_loss: 0.3497 - val_accuracy: 0.8692\n",
      "Epoch 388/500\n",
      "16908/16908 [==============================] - 51003s 3s/step - loss: 0.1078 - accuracy: 0.9624 - val_loss: 0.3587 - val_accuracy: 0.8656\n",
      "Epoch 389/500\n",
      "16908/16908 [==============================] - 291s 17ms/step - loss: 0.1020 - accuracy: 0.9634 - val_loss: 0.3877 - val_accuracy: 0.8609\n",
      "Epoch 390/500\n",
      "16908/16908 [==============================] - 253s 15ms/step - loss: 0.1086 - accuracy: 0.9609 - val_loss: 0.3594 - val_accuracy: 0.8706\n",
      "Epoch 391/500\n",
      "16908/16908 [==============================] - 273s 16ms/step - loss: 0.1108 - accuracy: 0.9607 - val_loss: 0.3514 - val_accuracy: 0.8704\n",
      "Epoch 392/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.1068 - accuracy: 0.9610 - val_loss: 0.3390 - val_accuracy: 0.8765\n",
      "Epoch 393/500\n",
      "16908/16908 [==============================] - 299s 18ms/step - loss: 0.1065 - accuracy: 0.9622 - val_loss: 0.3347 - val_accuracy: 0.8777\n",
      "Epoch 394/500\n",
      "16908/16908 [==============================] - 308s 18ms/step - loss: 0.1029 - accuracy: 0.9657 - val_loss: 0.3403 - val_accuracy: 0.8739\n",
      "Epoch 395/500\n",
      "16908/16908 [==============================] - 279s 17ms/step - loss: 0.1091 - accuracy: 0.9604 - val_loss: 0.3370 - val_accuracy: 0.8734\n",
      "Epoch 396/500\n",
      "16908/16908 [==============================] - 313s 18ms/step - loss: 0.1073 - accuracy: 0.9616 - val_loss: 0.3771 - val_accuracy: 0.8649\n",
      "Epoch 397/500\n",
      "16908/16908 [==============================] - 276s 16ms/step - loss: 0.0966 - accuracy: 0.9641 - val_loss: 0.3416 - val_accuracy: 0.8749\n",
      "Epoch 398/500\n",
      "16908/16908 [==============================] - 288s 17ms/step - loss: 0.1084 - accuracy: 0.9608 - val_loss: 0.3443 - val_accuracy: 0.8720\n",
      "Epoch 399/500\n",
      "16908/16908 [==============================] - 291s 17ms/step - loss: 0.1000 - accuracy: 0.9644 - val_loss: 0.3529 - val_accuracy: 0.8706\n",
      "Epoch 400/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.0992 - accuracy: 0.9647 - val_loss: 0.3408 - val_accuracy: 0.8711\n",
      "Epoch 401/500\n",
      "16908/16908 [==============================] - 317s 19ms/step - loss: 0.0990 - accuracy: 0.9662 - val_loss: 0.3451 - val_accuracy: 0.8722\n",
      "Epoch 402/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.1024 - accuracy: 0.9634 - val_loss: 0.3371 - val_accuracy: 0.8777\n",
      "Epoch 403/500\n",
      "16908/16908 [==============================] - 301s 18ms/step - loss: 0.0980 - accuracy: 0.9638 - val_loss: 0.3369 - val_accuracy: 0.8741\n",
      "Epoch 404/500\n",
      "16908/16908 [==============================] - 298s 18ms/step - loss: 0.1047 - accuracy: 0.9624 - val_loss: 0.3562 - val_accuracy: 0.8715\n",
      "Epoch 405/500\n",
      "16908/16908 [==============================] - 314s 19ms/step - loss: 0.1003 - accuracy: 0.9661 - val_loss: 0.3610 - val_accuracy: 0.8682\n",
      "Epoch 406/500\n",
      "16908/16908 [==============================] - 303s 18ms/step - loss: 0.0995 - accuracy: 0.9640 - val_loss: 0.3566 - val_accuracy: 0.8770\n",
      "Epoch 407/500\n",
      "16908/16908 [==============================] - 282s 17ms/step - loss: 0.0932 - accuracy: 0.9671 - val_loss: 0.3790 - val_accuracy: 0.8654\n",
      "Epoch 408/500\n",
      "16908/16908 [==============================] - 245s 14ms/step - loss: 0.0931 - accuracy: 0.9676 - val_loss: 0.3580 - val_accuracy: 0.8704\n",
      "Epoch 409/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.0931 - accuracy: 0.9671 - val_loss: 0.3581 - val_accuracy: 0.8722\n",
      "Epoch 410/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.0978 - accuracy: 0.9663 - val_loss: 0.3623 - val_accuracy: 0.8708\n",
      "Epoch 411/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.0946 - accuracy: 0.9661 - val_loss: 0.3668 - val_accuracy: 0.8666\n",
      "Epoch 412/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.0913 - accuracy: 0.9671 - val_loss: 0.3567 - val_accuracy: 0.8730\n",
      "Epoch 413/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.0930 - accuracy: 0.9674 - val_loss: 0.3692 - val_accuracy: 0.8659\n",
      "Epoch 414/500\n",
      "16908/16908 [==============================] - 271s 16ms/step - loss: 0.0893 - accuracy: 0.9691 - val_loss: 0.3467 - val_accuracy: 0.8772\n",
      "Epoch 415/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.0853 - accuracy: 0.9698 - val_loss: 0.3318 - val_accuracy: 0.8777\n",
      "Epoch 416/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.0901 - accuracy: 0.9682 - val_loss: 0.3549 - val_accuracy: 0.8718\n",
      "Epoch 417/500\n",
      "16908/16908 [==============================] - 268s 16ms/step - loss: 0.0875 - accuracy: 0.9708 - val_loss: 0.3546 - val_accuracy: 0.8758\n",
      "Epoch 418/500\n",
      "16908/16908 [==============================] - 305s 18ms/step - loss: 0.0906 - accuracy: 0.9675 - val_loss: 0.3741 - val_accuracy: 0.8666\n",
      "Epoch 419/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.0889 - accuracy: 0.9697 - val_loss: 0.3320 - val_accuracy: 0.8763\n",
      "Epoch 420/500\n",
      "16908/16908 [==============================] - 267s 16ms/step - loss: 0.0808 - accuracy: 0.9725 - val_loss: 0.3368 - val_accuracy: 0.8746\n",
      "Epoch 421/500\n",
      "16908/16908 [==============================] - 294s 17ms/step - loss: 0.0823 - accuracy: 0.9718 - val_loss: 0.3447 - val_accuracy: 0.8779\n",
      "Epoch 422/500\n",
      "16908/16908 [==============================] - 251s 15ms/step - loss: 0.0841 - accuracy: 0.9702 - val_loss: 0.3430 - val_accuracy: 0.8784\n",
      "Epoch 423/500\n",
      "16908/16908 [==============================] - 314s 19ms/step - loss: 0.0901 - accuracy: 0.9689 - val_loss: 0.3333 - val_accuracy: 0.8808\n",
      "Epoch 424/500\n",
      "16908/16908 [==============================] - 253s 15ms/step - loss: 0.0803 - accuracy: 0.9720 - val_loss: 0.3392 - val_accuracy: 0.8791\n",
      "Epoch 425/500\n",
      "16908/16908 [==============================] - 263s 16ms/step - loss: 0.0770 - accuracy: 0.9746 - val_loss: 0.3486 - val_accuracy: 0.8812\n",
      "Epoch 426/500\n",
      "16908/16908 [==============================] - 251s 15ms/step - loss: 0.0822 - accuracy: 0.9718 - val_loss: 0.3318 - val_accuracy: 0.8791\n",
      "Epoch 427/500\n",
      "16908/16908 [==============================] - 266s 16ms/step - loss: 0.0828 - accuracy: 0.9703 - val_loss: 0.3630 - val_accuracy: 0.8696\n",
      "Epoch 428/500\n",
      "16908/16908 [==============================] - 264s 16ms/step - loss: 0.0825 - accuracy: 0.9705 - val_loss: 0.3338 - val_accuracy: 0.8763\n",
      "Epoch 429/500\n",
      "16908/16908 [==============================] - 253s 15ms/step - loss: 0.0834 - accuracy: 0.9710 - val_loss: 0.3452 - val_accuracy: 0.8765\n",
      "Epoch 430/500\n",
      "16908/16908 [==============================] - 250s 15ms/step - loss: 0.0847 - accuracy: 0.9692 - val_loss: 0.3467 - val_accuracy: 0.8789\n",
      "Epoch 431/500\n",
      "16908/16908 [==============================] - 255s 15ms/step - loss: 0.0800 - accuracy: 0.9728 - val_loss: 0.3346 - val_accuracy: 0.8829\n",
      "Epoch 432/500\n",
      "16908/16908 [==============================] - 263s 16ms/step - loss: 0.0797 - accuracy: 0.9717 - val_loss: 0.3593 - val_accuracy: 0.8689\n",
      "Epoch 433/500\n",
      "16908/16908 [==============================] - 339s 20ms/step - loss: 0.0793 - accuracy: 0.9717 - val_loss: 0.3379 - val_accuracy: 0.8789\n",
      "Epoch 434/500\n",
      "16908/16908 [==============================] - 310s 18ms/step - loss: 0.0759 - accuracy: 0.9741 - val_loss: 0.3512 - val_accuracy: 0.8727\n",
      "Epoch 435/500\n",
      "16908/16908 [==============================] - 377s 22ms/step - loss: 0.0739 - accuracy: 0.9746 - val_loss: 0.3359 - val_accuracy: 0.8812\n",
      "Epoch 436/500\n",
      "16908/16908 [==============================] - 317s 19ms/step - loss: 0.0777 - accuracy: 0.9733 - val_loss: 0.3388 - val_accuracy: 0.8779\n",
      "Epoch 437/500\n",
      "16908/16908 [==============================] - 296s 18ms/step - loss: 0.0758 - accuracy: 0.9735 - val_loss: 0.3459 - val_accuracy: 0.8725\n",
      "Epoch 438/500\n",
      "16908/16908 [==============================] - 280s 17ms/step - loss: 0.0716 - accuracy: 0.9766 - val_loss: 0.3366 - val_accuracy: 0.8786\n",
      "Epoch 439/500\n",
      "16908/16908 [==============================] - 289s 17ms/step - loss: 0.0716 - accuracy: 0.9771 - val_loss: 0.3322 - val_accuracy: 0.8782\n",
      "Epoch 440/500\n",
      "16908/16908 [==============================] - 296s 18ms/step - loss: 0.0752 - accuracy: 0.9737 - val_loss: 0.3497 - val_accuracy: 0.8720\n",
      "Epoch 441/500\n",
      "16908/16908 [==============================] - 280s 17ms/step - loss: 0.0754 - accuracy: 0.9743 - val_loss: 0.3295 - val_accuracy: 0.8770\n",
      "Epoch 442/500\n",
      "16908/16908 [==============================] - 373s 22ms/step - loss: 0.0725 - accuracy: 0.9758 - val_loss: 0.3381 - val_accuracy: 0.8805\n",
      "Epoch 443/500\n",
      "16908/16908 [==============================] - 346s 20ms/step - loss: 0.0705 - accuracy: 0.9749 - val_loss: 0.3389 - val_accuracy: 0.8770\n",
      "Epoch 444/500\n",
      "16908/16908 [==============================] - 314s 19ms/step - loss: 0.0699 - accuracy: 0.9766 - val_loss: 0.3412 - val_accuracy: 0.8763\n",
      "Epoch 445/500\n",
      "16908/16908 [==============================] - 286s 17ms/step - loss: 0.0743 - accuracy: 0.9731 - val_loss: 0.3468 - val_accuracy: 0.8725\n",
      "Epoch 446/500\n",
      "16908/16908 [==============================] - 303s 18ms/step - loss: 0.0761 - accuracy: 0.9742 - val_loss: 0.3229 - val_accuracy: 0.8777\n",
      "Epoch 447/500\n",
      "16908/16908 [==============================] - 303s 18ms/step - loss: 0.0736 - accuracy: 0.9741 - val_loss: 0.3451 - val_accuracy: 0.8753\n",
      "Epoch 448/500\n",
      "16908/16908 [==============================] - 297s 18ms/step - loss: 0.0702 - accuracy: 0.9758 - val_loss: 0.3428 - val_accuracy: 0.8779\n",
      "Epoch 449/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.0715 - accuracy: 0.9749 - val_loss: 0.3439 - val_accuracy: 0.8808\n",
      "Epoch 450/500\n",
      "16908/16908 [==============================] - 293s 17ms/step - loss: 0.0735 - accuracy: 0.9743 - val_loss: 0.3555 - val_accuracy: 0.8758\n",
      "Epoch 451/500\n",
      "16908/16908 [==============================] - 296s 18ms/step - loss: 0.0748 - accuracy: 0.9738 - val_loss: 0.3334 - val_accuracy: 0.8777\n",
      "Epoch 452/500\n",
      "16908/16908 [==============================] - 292s 17ms/step - loss: 0.0691 - accuracy: 0.9767 - val_loss: 0.3368 - val_accuracy: 0.8777\n",
      "Epoch 453/500\n",
      "16908/16908 [==============================] - 291s 17ms/step - loss: 0.0730 - accuracy: 0.9740 - val_loss: 0.3573 - val_accuracy: 0.8765\n",
      "Epoch 454/500\n",
      "16908/16908 [==============================] - 307s 18ms/step - loss: 0.0724 - accuracy: 0.9743 - val_loss: 0.3246 - val_accuracy: 0.8822\n",
      "Epoch 455/500\n",
      "16908/16908 [==============================] - 293s 17ms/step - loss: 0.0718 - accuracy: 0.9748 - val_loss: 0.3270 - val_accuracy: 0.8801\n",
      "Epoch 456/500\n",
      "16908/16908 [==============================] - 321s 19ms/step - loss: 0.0683 - accuracy: 0.9772 - val_loss: 0.3388 - val_accuracy: 0.8760\n",
      "Epoch 457/500\n",
      "16908/16908 [==============================] - 285s 17ms/step - loss: 0.0638 - accuracy: 0.9786 - val_loss: 0.3353 - val_accuracy: 0.8817\n",
      "Epoch 458/500\n",
      "16908/16908 [==============================] - 304s 18ms/step - loss: 0.0684 - accuracy: 0.9766 - val_loss: 0.3255 - val_accuracy: 0.8815\n",
      "Epoch 459/500\n",
      "16908/16908 [==============================] - 356s 21ms/step - loss: 0.0695 - accuracy: 0.9763 - val_loss: 0.3301 - val_accuracy: 0.8815\n",
      "Epoch 460/500\n",
      "16908/16908 [==============================] - 335s 20ms/step - loss: 0.0696 - accuracy: 0.9762 - val_loss: 0.3299 - val_accuracy: 0.8836\n",
      "Epoch 461/500\n",
      "16908/16908 [==============================] - 302s 18ms/step - loss: 0.0633 - accuracy: 0.9778 - val_loss: 0.3246 - val_accuracy: 0.8824\n",
      "Epoch 462/500\n",
      "16908/16908 [==============================] - 390s 23ms/step - loss: 0.0664 - accuracy: 0.9768 - val_loss: 0.3433 - val_accuracy: 0.8808\n",
      "Epoch 463/500\n",
      "16908/16908 [==============================] - 325s 19ms/step - loss: 0.0636 - accuracy: 0.9785 - val_loss: 0.3254 - val_accuracy: 0.8853\n",
      "Epoch 464/500\n",
      "16908/16908 [==============================] - 314s 19ms/step - loss: 0.0629 - accuracy: 0.9782 - val_loss: 0.3473 - val_accuracy: 0.8786\n",
      "Epoch 465/500\n",
      "16908/16908 [==============================] - 282s 17ms/step - loss: 0.0671 - accuracy: 0.9771 - val_loss: 0.3301 - val_accuracy: 0.8827\n",
      "Epoch 466/500\n",
      "16908/16908 [==============================] - 296s 17ms/step - loss: 0.0656 - accuracy: 0.9767 - val_loss: 0.3385 - val_accuracy: 0.8812\n",
      "Epoch 467/500\n",
      "16908/16908 [==============================] - 297s 18ms/step - loss: 0.0636 - accuracy: 0.9780 - val_loss: 0.3368 - val_accuracy: 0.8829\n",
      "Epoch 468/500\n",
      "16908/16908 [==============================] - 327s 19ms/step - loss: 0.0639 - accuracy: 0.9779 - val_loss: 0.3431 - val_accuracy: 0.8784\n",
      "Epoch 469/500\n",
      "16908/16908 [==============================] - 280s 17ms/step - loss: 0.0678 - accuracy: 0.9759 - val_loss: 0.4166 - val_accuracy: 0.8614\n",
      "Epoch 470/500\n",
      "16908/16908 [==============================] - 307s 18ms/step - loss: 0.0652 - accuracy: 0.9766 - val_loss: 0.3260 - val_accuracy: 0.8782\n",
      "Epoch 471/500\n",
      "16908/16908 [==============================] - 342s 20ms/step - loss: 0.0629 - accuracy: 0.9787 - val_loss: 0.3586 - val_accuracy: 0.8746\n",
      "Epoch 472/500\n",
      "16908/16908 [==============================] - 333s 20ms/step - loss: 0.0612 - accuracy: 0.9784 - val_loss: 0.3403 - val_accuracy: 0.8763\n",
      "Epoch 473/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.0648 - accuracy: 0.9778 - val_loss: 0.3417 - val_accuracy: 0.8782\n",
      "Epoch 474/500\n",
      "16908/16908 [==============================] - 287s 17ms/step - loss: 0.0624 - accuracy: 0.9786 - val_loss: 0.3434 - val_accuracy: 0.8796\n",
      "Epoch 475/500\n",
      "16908/16908 [==============================] - 308s 18ms/step - loss: 0.0620 - accuracy: 0.9792 - val_loss: 0.3319 - val_accuracy: 0.8805\n",
      "Epoch 476/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.0599 - accuracy: 0.9805 - val_loss: 0.3390 - val_accuracy: 0.8831\n",
      "Epoch 477/500\n",
      "16908/16908 [==============================] - 326s 19ms/step - loss: 0.0643 - accuracy: 0.9778 - val_loss: 0.3456 - val_accuracy: 0.8779\n",
      "Epoch 478/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.0641 - accuracy: 0.9779 - val_loss: 0.3488 - val_accuracy: 0.8798\n",
      "Epoch 479/500\n",
      "16908/16908 [==============================] - 326s 19ms/step - loss: 0.0640 - accuracy: 0.9785 - val_loss: 0.3470 - val_accuracy: 0.8815\n",
      "Epoch 480/500\n",
      "16908/16908 [==============================] - 329s 19ms/step - loss: 0.0622 - accuracy: 0.9796 - val_loss: 0.3374 - val_accuracy: 0.8836\n",
      "Epoch 481/500\n",
      "16908/16908 [==============================] - 363s 21ms/step - loss: 0.0569 - accuracy: 0.9801 - val_loss: 0.3479 - val_accuracy: 0.8782\n",
      "Epoch 482/500\n",
      "16908/16908 [==============================] - 361s 21ms/step - loss: 0.0592 - accuracy: 0.9802 - val_loss: 0.3376 - val_accuracy: 0.8817\n",
      "Epoch 483/500\n",
      "16908/16908 [==============================] - 364s 22ms/step - loss: 0.0586 - accuracy: 0.9805 - val_loss: 0.3297 - val_accuracy: 0.8817\n",
      "Epoch 484/500\n",
      "16908/16908 [==============================] - 367s 22ms/step - loss: 0.0591 - accuracy: 0.9810 - val_loss: 0.3382 - val_accuracy: 0.8803\n",
      "Epoch 485/500\n",
      "16908/16908 [==============================] - 359s 21ms/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.3443 - val_accuracy: 0.8782\n",
      "Epoch 486/500\n",
      "16908/16908 [==============================] - 358s 21ms/step - loss: 0.0595 - accuracy: 0.9800 - val_loss: 0.3460 - val_accuracy: 0.8760\n",
      "Epoch 487/500\n",
      "16908/16908 [==============================] - 300s 18ms/step - loss: 0.0547 - accuracy: 0.9810 - val_loss: 0.3316 - val_accuracy: 0.8791\n",
      "Epoch 488/500\n",
      "16908/16908 [==============================] - 309s 18ms/step - loss: 0.0550 - accuracy: 0.9813 - val_loss: 0.3221 - val_accuracy: 0.8850\n",
      "Epoch 489/500\n",
      "16908/16908 [==============================] - 314s 19ms/step - loss: 0.0548 - accuracy: 0.9818 - val_loss: 0.3362 - val_accuracy: 0.8796\n",
      "Epoch 490/500\n",
      "16908/16908 [==============================] - 319s 19ms/step - loss: 0.0527 - accuracy: 0.9821 - val_loss: 0.3240 - val_accuracy: 0.8805\n",
      "Epoch 491/500\n",
      "16908/16908 [==============================] - 291s 17ms/step - loss: 0.0570 - accuracy: 0.9806 - val_loss: 0.3263 - val_accuracy: 0.8791\n",
      "Epoch 492/500\n",
      "16908/16908 [==============================] - 270s 16ms/step - loss: 0.0502 - accuracy: 0.9835 - val_loss: 0.3198 - val_accuracy: 0.8793\n",
      "Epoch 493/500\n",
      "16908/16908 [==============================] - 274s 16ms/step - loss: 0.0559 - accuracy: 0.9817 - val_loss: 0.3315 - val_accuracy: 0.8808\n",
      "Epoch 494/500\n",
      "16908/16908 [==============================] - 293s 17ms/step - loss: 0.0523 - accuracy: 0.9824 - val_loss: 0.3513 - val_accuracy: 0.8758\n",
      "Epoch 495/500\n",
      "16908/16908 [==============================] - 308s 18ms/step - loss: 0.0534 - accuracy: 0.9815 - val_loss: 0.3238 - val_accuracy: 0.8827\n",
      "Epoch 496/500\n",
      "16908/16908 [==============================] - 297s 18ms/step - loss: 0.0520 - accuracy: 0.9828 - val_loss: 0.3375 - val_accuracy: 0.8815\n",
      "Epoch 497/500\n",
      "16908/16908 [==============================] - 311s 18ms/step - loss: 0.0522 - accuracy: 0.9820 - val_loss: 0.3320 - val_accuracy: 0.8838\n",
      "Epoch 498/500\n",
      "16908/16908 [==============================] - 341s 20ms/step - loss: 0.0533 - accuracy: 0.9825 - val_loss: 0.3234 - val_accuracy: 0.8805\n",
      "Epoch 499/500\n",
      "16908/16908 [==============================] - 297s 18ms/step - loss: 0.0534 - accuracy: 0.9810 - val_loss: 0.3366 - val_accuracy: 0.8822\n",
      "Epoch 500/500\n",
      "16908/16908 [==============================] - 324s 19ms/step - loss: 0.0531 - accuracy: 0.9824 - val_loss: 0.3201 - val_accuracy: 0.8831\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=20, min_lr=0.000001)\n",
    "\n",
    "# Please change the model name accordingly.\n",
    "model_save = ModelCheckpoint('Augmented_Model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=500,\n",
    "                     validation_data=(x_testcnn, y_test), callbacks=[model_save, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the Train Valid Loss Graph\n",
    "\n",
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model.json\n",
    "\n",
    "import json\n",
    "model_json = model.to_json()\n",
    "with open(\"MLModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "accuracy: 87.93%\n"
     ]
    }
   ],
   "source": [
    "# loading json and creating model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('MLModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Augmented_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_testcnn, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4227/4227 [==============================] - 24s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 1, 0, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = loaded_model.predict(x_testcnn, \n",
    "                         batch_size=16, \n",
    "                         verbose=1)\n",
    "\n",
    "preds=preds.argmax(axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions \n",
    "preds = preds.astype(int).flatten()\n",
    "preds = (lb.inverse_transform((preds)))\n",
    "preds = pd.DataFrame({'predicted_values': preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual labels\n",
    "test_valid_lb = np.array(df.labels)\n",
    "lb = LabelEncoder()\n",
    "test_valid_lb = np_utils.to_categorical(lb.fit_transform(test_valid_lb))\n",
    "actual=test_valid_lb.argmax(axis=1)\n",
    "actual = actual.astype(int).flatten()\n",
    "actual = (lb.inverse_transform((actual)))\n",
    "actual = pd.DataFrame({'actual_values': actual})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_values</th>\n",
       "      <th>predicted_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual_values predicted_values\n",
       "170      negative         positive\n",
       "171      negative         negative\n",
       "172      negative         negative\n",
       "173      negative         positive\n",
       "174      negative          neutral\n",
       "175      negative          neutral\n",
       "176      negative         negative\n",
       "177      negative         positive\n",
       "178      negative         positive\n",
       "179      negative         negative"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets combined both of them into a single dataframe\n",
    "finaldf = actual.join(preds)\n",
    "finaldf[170:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_values</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  actual_values\n",
       "predicted_values               \n",
       "negative                   1619\n",
       "neutral                    1054\n",
       "positive                   1554"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.groupby('predicted_values').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
